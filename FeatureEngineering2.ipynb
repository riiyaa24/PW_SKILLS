{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale numerical features to a fixed range, typically between 0 and 1. This scaling method preserves the original distribution of the data while ensuring that all features have the same scale. Min-Max scaling is particularly useful when working with algorithms that require input features to be on a similar scale, such as neural networks, support vector machines, and k-nearest neighbors.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "Where:\n",
    "- \\(X\\) is the original feature value.\n",
    "- \\(X_{\\text{min}}\\) is the minimum value of the feature in the dataset.\n",
    "- \\(X_{\\text{max}}\\) is the maximum value of the feature in the dataset.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose we have a dataset with a numerical feature representing the age of individuals. The age values range from 20 to 60 years. We want to scale these age values using Min-Max scaling.\n",
    "\n",
    "Original age values:\n",
    "- Person 1: 20 years\n",
    "- Person 2: 35 years\n",
    "- Person 3: 50 years\n",
    "- Person 4: 60 years\n",
    "\n",
    "To apply Min-Max scaling:\n",
    "1. Calculate the minimum (\\(X_{\\text{min}}\\)) and maximum (\\(X_{\\text{max}}\\)) values of the age feature:\n",
    "   - \\(X_{\\text{min}} = 20\\) years\n",
    "   - \\(X_{\\text{max}} = 60\\) years\n",
    "\n",
    "2. Use the Min-Max scaling formula to scale each age value:\n",
    "   - For Person 1: \\(X_{\\text{scaled}} = \\frac{20 - 20}{60 - 20} = \\frac{0}{40} = 0\\)\n",
    "   - For Person 2: \\(X_{\\text{scaled}} = \\frac{35 - 20}{60 - 20} = \\frac{15}{40} = 0.375\\)\n",
    "   - For Person 3: \\(X_{\\text{scaled}} = \\frac{50 - 20}{60 - 20} = \\frac{30}{40} = 0.75\\)\n",
    "   - For Person 4: \\(X_{\\text{scaled}} = \\frac{60 - 20}{60 - 20} = \\frac{40}{40} = 1\\)\n",
    "\n",
    "The scaled age values after Min-Max scaling are:\n",
    "- Person 1: 0\n",
    "- Person 2: 0.375\n",
    "- Person 3: 0.75\n",
    "- Person 4: 1\n",
    "\n",
    "Now, all age values are scaled to the range [0, 1], making them suitable for use in machine learning algorithms that require standardized feature scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "The Unit Vector technique, also known as unit normalization or vector normalization, is a feature scaling method used to rescale numerical features such that each feature vector has a length of 1. This technique is commonly applied in machine learning algorithms that rely on vector operations, such as clustering algorithms and similarity-based models.\n",
    "\n",
    "The formula for Unit Vector scaling is:\n",
    "\n",
    "\\[ X_{\\text{unit}} = \\frac{X}{\\|X\\|} \\]\n",
    "\n",
    "Where:\n",
    "- \\(X\\) is the original feature vector.\n",
    "- \\(\\|X\\|\\) represents the Euclidean norm or magnitude of the feature vector.\n",
    "\n",
    "Unit Vector scaling rescales each feature vector by dividing it by its Euclidean norm, ensuring that the resulting vector has a length of 1. This normalization technique preserves the direction of the feature vectors while standardizing their magnitude.\n",
    "\n",
    "Here's how Unit Vector scaling differs from Min-Max scaling:\n",
    "\n",
    "1. **Scale**:\n",
    "   - Min-Max scaling rescales feature values to a fixed range, typically between 0 and 1, without altering their direction.\n",
    "   - Unit Vector scaling rescales feature vectors to have a length of 1, preserving their direction but standardizing their magnitude.\n",
    "\n",
    "2. **Range**:\n",
    "   - Min-Max scaling ensures that feature values are bounded within a specific range.\n",
    "   - Unit Vector scaling ensures that feature vectors have a length of 1, regardless of the original range of feature values.\n",
    "\n",
    "3. **Use Case**:\n",
    "   - Min-Max scaling is commonly used when the magnitude of feature values is meaningful and should be preserved within a fixed range.\n",
    "   - Unit Vector scaling is often used in scenarios where the direction of feature vectors is more important than their magnitude, such as in similarity-based calculations or clustering algorithms.\n",
    "\n",
    "Now, let's illustrate the application of Unit Vector scaling with an example:\n",
    "\n",
    "Suppose we have a dataset with two numerical features representing the length and width of rectangles. We want to scale these features using Unit Vector scaling.\n",
    "\n",
    "Original feature vectors:\n",
    "- Rectangle 1: (length, width) = (3, 4)\n",
    "- Rectangle 2: (length, width) = (5, 12)\n",
    "\n",
    "To apply Unit Vector scaling:\n",
    "1. Calculate the Euclidean norm (\\(\\|X\\|\\)) of each feature vector:\n",
    "   - For Rectangle 1: \\(\\|X\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\\)\n",
    "   - For Rectangle 2: \\(\\|X\\| = \\sqrt{5^2 + 12^2} = \\sqrt{25 + 144} = \\sqrt{169} = 13\\)\n",
    "\n",
    "2. Use the Unit Vector scaling formula to scale each feature vector:\n",
    "   - For Rectangle 1: \\(\\frac{(3, 4)}{5} = \\left(\\frac{3}{5}, \\frac{4}{5}\\right)\\)\n",
    "   - For Rectangle 2: \\(\\frac{(5, 12)}{13} = \\left(\\frac{5}{13}, \\frac{12}{13}\\right)\\)\n",
    "\n",
    "The scaled feature vectors after Unit Vector scaling are:\n",
    "- Rectangle 1: \\(\\left(\\frac{3}{5}, \\frac{4}{5}\\right)\\)\n",
    "- Rectangle 2: \\(\\left(\\frac{5}{13}, \\frac{12}{13}\\right)\\)\n",
    "\n",
    "Now, both feature vectors have a length of 1, preserving their direction while standardizing their magnitude for further analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving the most important information. PCA achieves this by identifying the principal components, which are linear combinations of the original features, that capture the maximum variance in the data. These principal components represent the directions along which the data varies the most.\n",
    "\n",
    "Here's how PCA works:\n",
    "\n",
    "1. **Compute the Covariance Matrix**:\n",
    "   - Given a dataset with \\(n\\) observations and \\(m\\) features, compute the \\(m \\times m\\) covariance matrix \\(\\Sigma\\) to capture the relationships between different features.\n",
    "\n",
    "2. **Calculate Eigenvectors and Eigenvalues**:\n",
    "   - Compute the eigenvectors \\(v_1, v_2, ..., v_m\\) and corresponding eigenvalues \\(\\lambda_1, \\lambda_2, ..., \\lambda_m\\) of the covariance matrix \\(\\Sigma\\).\n",
    "   - Eigenvectors represent the directions (principal components) of maximum variance in the data, while eigenvalues represent the magnitude of variance along each eigenvector.\n",
    "\n",
    "3. **Select Principal Components**:\n",
    "   - Sort the eigenvectors based on their corresponding eigenvalues in descending order.\n",
    "   - Choose the \\(k\\) eigenvectors (principal components) corresponding to the \\(k\\) largest eigenvalues to form the reduced-dimensional subspace.\n",
    "\n",
    "4. **Transform Data**:\n",
    "   - Project the original data onto the subspace spanned by the selected principal components.\n",
    "   - Multiply the original data matrix by the matrix of selected eigenvectors to obtain the lower-dimensional representation of the data.\n",
    "\n",
    "PCA is commonly used in various applications, including data visualization, noise reduction, and feature extraction. By reducing the dimensionality of the data while preserving the most important information, PCA can simplify the analysis of high-dimensional datasets and improve the performance of machine learning algorithms.\n",
    "\n",
    "Now, let's illustrate the application of PCA with an example:\n",
    "\n",
    "Suppose we have a dataset containing measurements of fruits, including their weight, width, and height. We want to reduce the dimensionality of the dataset using PCA.\n",
    "\n",
    "Original dataset:\n",
    "- Fruit 1: (weight, width, height) = (100g, 5cm, 8cm)\n",
    "- Fruit 2: (weight, width, height) = (150g, 6cm, 10cm)\n",
    "- Fruit 3: (weight, width, height) = (120g, 4cm, 9cm)\n",
    "\n",
    "To apply PCA:\n",
    "1. Compute the covariance matrix:\n",
    "   \\[ \\Sigma = \\begin{bmatrix} 250 & 12.5 & 25 \\\\ 12.5 & 0.5 & 0.75 \\\\ 25 & 0.75 & 1 \\end{bmatrix} \\]\n",
    "\n",
    "2. Calculate eigenvectors and eigenvalues:\n",
    "   - Eigenvectors:\n",
    "     \\[ v_1 = \\begin{bmatrix} 0.99 \\\\ 0.08 \\\\ 0.14 \\end{bmatrix}, \\quad v_2 = \\begin{bmatrix} -0.13 \\\\ 0.71 \\\\ 0.69 \\end{bmatrix}, \\quad v_3 = \\begin{bmatrix} 0.07 \\\\ -0.70 \\\\ 0.71 \\end{bmatrix} \\]\n",
    "   - Eigenvalues:\n",
    "     \\[ \\lambda_1 = 250.17, \\quad \\lambda_2 = 1.18, \\quad \\lambda_3 = 0.14 \\]\n",
    "\n",
    "3. Select principal components:\n",
    "   - Choose the first two principal components (PC1 and PC2) corresponding to the two largest eigenvalues.\n",
    "\n",
    "4. Transform data:\n",
    "   - Project the original data onto the subspace spanned by PC1 and PC2 to obtain the reduced-dimensional representation of the data.\n",
    "\n",
    "The transformed dataset after PCA will have two dimensions, representing the principal components capturing the most significant variance in the original data. This reduced-dimensional representation can be used for further analysis or visualization of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "Principal Component Analysis (PCA) is closely related to feature extraction, as it can be used to extract new features (principal components) from high-dimensional data that capture the most important information while reducing the dimensionality of the dataset. Feature extraction aims to transform the original features into a lower-dimensional space, where each new feature (principal component) is a linear combination of the original features.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "1. **Dimensionality Reduction**:\n",
    "   - PCA identifies the principal components, which are orthogonal vectors that represent the directions of maximum variance in the data.\n",
    "   - By selecting a subset of principal components, PCA reduces the dimensionality of the dataset while preserving the most important information.\n",
    "\n",
    "2. **Linear Combination of Features**:\n",
    "   - Each principal component is a linear combination of the original features, where the coefficients of the linear combination are given by the corresponding eigenvectors of the covariance matrix.\n",
    "   - PCA transforms the original feature space into a new space spanned by the principal components, where each new feature represents a different direction of variation in the data.\n",
    "\n",
    "3. **Feature Extraction**:\n",
    "   - The principal components extracted by PCA can be interpreted as new features that capture the most significant patterns or structures in the data.\n",
    "   - These new features may be more informative and less redundant than the original features, making them suitable for use in machine learning algorithms or further analysis.\n",
    "\n",
    "4. **Reduced-Dimensional Representation**:\n",
    "   - PCA provides a reduced-dimensional representation of the dataset, where the number of features is significantly smaller than the original number of features.\n",
    "   - This reduced-dimensional representation simplifies the analysis of high-dimensional datasets and improves the computational efficiency of machine learning algorithms.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Suppose we have a dataset containing grayscale images of handwritten digits, where each image is represented as a vector of pixel intensities (e.g., 28x28 = 784 dimensions). We want to extract new features that capture the most important patterns in the images while reducing the dimensionality of the dataset.\n",
    "\n",
    "Using PCA:\n",
    "1. Apply PCA to the dataset to identify the principal components (eigenimages) that represent the directions of maximum variance in the images.\n",
    "2. Select a subset of principal components that capture most of the variance in the dataset (e.g., the top k components).\n",
    "3. Transform the original images into the reduced-dimensional space spanned by the selected principal components.\n",
    "4. The transformed images are represented as new features (principal components) that capture the most significant patterns in the images while reducing the dimensionality of the dataset.\n",
    "\n",
    "These new features extracted by PCA can be used for tasks such as image classification or clustering, where the reduced-dimensional representation simplifies the analysis and improves the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling, you would follow these steps:\n",
    "\n",
    "1. **Understand the Dataset**:\n",
    "   - Thoroughly examine the dataset to understand the features available and their distributions.\n",
    "   - Identify the numerical features that require scaling, such as price, rating, and delivery time.\n",
    "\n",
    "2. **Compute Min and Max Values**:\n",
    "   - For each numerical feature, calculate the minimum and maximum values present in the dataset.\n",
    "   - For example, find the minimum and maximum prices, ratings, and delivery times across all data points.\n",
    "\n",
    "3. **Apply Min-Max Scaling**:\n",
    "   - Use the Min-Max scaling formula to scale each numerical feature to a fixed range, typically between 0 and 1.\n",
    "   - For each feature \\(X\\), apply the following transformation:\n",
    "     \\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "   - This ensures that the scaled values fall within the range [0, 1], making them comparable and suitable for use in the recommendation system.\n",
    "\n",
    "4. **Update the Dataset**:\n",
    "   - Replace the original numerical values with their scaled counterparts in the dataset.\n",
    "   - Each numerical feature will now be represented by its scaled value, ensuring consistency and comparability across features.\n",
    "\n",
    "5. **Validation and Testing**:\n",
    "   - Validate the scaled dataset to ensure that the scaling process has been applied correctly.\n",
    "   - Perform testing to verify that the scaled features are appropriate for use in building the recommendation system.\n",
    "\n",
    "6. **Model Building**:\n",
    "   - Use the preprocessed dataset with scaled features to train the recommendation system model.\n",
    "   - Incorporate other relevant features and techniques as needed to enhance the model's performance and accuracy.\n",
    "\n",
    "By using Min-Max scaling to preprocess the data, you ensure that numerical features such as price, rating, and delivery time are standardized to a common scale, making them suitable for input into the recommendation system. This scaling process helps in improving the model's interpretability, convergence speed, and overall performance by removing the influence of varying feature scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "To use Principal Component Analysis (PCA) to reduce the dimensionality of a dataset containing features for predicting stock prices, you would follow these steps:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Clean the dataset by handling missing values, outliers, and any other inconsistencies.\n",
    "   - Standardize or normalize the numerical features to ensure they have a similar scale, as PCA is sensitive to the scale of the features.\n",
    "\n",
    "2. **Feature Selection** (optional):\n",
    "   - If there are redundant or irrelevant features in the dataset, consider performing feature selection techniques to reduce the feature space before applying PCA.\n",
    "   - Feature selection can help improve the effectiveness and efficiency of PCA by focusing on the most informative features.\n",
    "\n",
    "3. **Compute the Covariance Matrix**:\n",
    "   - Calculate the covariance matrix of the standardized dataset. The covariance matrix captures the relationships between different features and is essential for PCA.\n",
    "\n",
    "4. **Calculate Eigenvectors and Eigenvalues**:\n",
    "   - Compute the eigenvectors and eigenvalues of the covariance matrix. These eigenvectors represent the principal components (PCs) and the amount of variance each PC captures, respectively.\n",
    "   - Eigenvectors with higher eigenvalues correspond to directions in the feature space that explain the most variance in the data.\n",
    "\n",
    "5. **Select Principal Components**:\n",
    "   - Determine the number of principal components (PCs) to retain based on the desired level of dimensionality reduction.\n",
    "   - You can either select a fixed number of PCs or choose the number of PCs that capture a certain percentage of the total variance (e.g., 90%).\n",
    "   - Typically, you would retain the top \\(k\\) eigenvectors corresponding to the \\(k\\) largest eigenvalues.\n",
    "\n",
    "6. **Transform Data**:\n",
    "   - Project the original data onto the subspace spanned by the selected principal components.\n",
    "   - Multiply the standardized dataset by the matrix of selected eigenvectors to obtain the reduced-dimensional representation of the data.\n",
    "\n",
    "7. **Model Training and Evaluation**:\n",
    "   - Use the reduced-dimensional dataset as input features for training machine learning models to predict stock prices.\n",
    "   - Evaluate the performance of the models using appropriate metrics and validation techniques.\n",
    "\n",
    "By applying PCA to the dataset, you reduce the dimensionality of the feature space while preserving as much of the original variance as possible. This can help mitigate the curse of dimensionality, improve computational efficiency, and potentially enhance the predictive performance of the models. Additionally, PCA can provide insights into the underlying structure of the data and identify the most significant patterns or trends affecting stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "To perform Min-Max scaling to transform the values in the dataset to a range of -1 to 1, you can follow these steps:\n",
    "\n",
    "1. **Calculate Min and Max Values**:\n",
    "   - Find the minimum (\\(X_{\\text{min}}\\)) and maximum (\\(X_{\\text{max}}\\)) values in the dataset.\n",
    "\n",
    "\\[ X_{\\text{min}} = 1 \\]\n",
    "\\[ X_{\\text{max}} = 20 \\]\n",
    "\n",
    "2. **Apply Min-Max Scaling**:\n",
    "   - Use the Min-Max scaling formula to scale each value in the dataset to the desired range of -1 to 1.\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\times (max_{\\text{scaled}} - min_{\\text{scaled}}) + min_{\\text{scaled}} \\]\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - 1}{20 - 1} \\times (1 - (-1)) + (-1) \\]\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - 1}{19} \\times 2 - 1 \\]\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - 1}{19} \\times 2 - 1 \\]\n",
    "\n",
    "3. **Transform the Values**:\n",
    "   - Apply the Min-Max scaling formula to each value in the dataset.\n",
    "\n",
    "For \\(X = 1\\):\n",
    "\\[ X_{\\text{scaled}} = \\frac{1 - 1}{19} \\times 2 - 1 = \\frac{0}{19} \\times 2 - 1 = -1 \\]\n",
    "\n",
    "For \\(X = 5\\):\n",
    "\\[ X_{\\text{scaled}} = \\frac{5 - 1}{19} \\times 2 - 1 = \\frac{4}{19} \\times 2 - 1 = -\\frac{11}{19} \\]\n",
    "\n",
    "For \\(X = 10\\):\n",
    "\\[ X_{\\text{scaled}} = \\frac{10 - 1}{19} \\times 2 - 1 = \\frac{9}{19} \\times 2 - 1 = -\\frac{1}{19} \\]\n",
    "\n",
    "For \\(X = 15\\):\n",
    "\\[ X_{\\text{scaled}} = \\frac{15 - 1}{19} \\times 2 - 1 = \\frac{14}{19} \\times 2 - 1 = \\frac{7}{19} \\]\n",
    "\n",
    "For \\(X = 20\\):\n",
    "\\[ X_{\\text{scaled}} = \\frac{20 - 1}{19} \\times 2 - 1 = \\frac{19}{19} \\times 2 - 1 = 1 \\]\n",
    "\n",
    "So, the Min-Max scaled values for the dataset [1, 5, 10, 15, 20] transformed to a range of -1 to 1 are [-1, -\\frac{11}{19}, -\\frac{1}{19}, \\frac{7}{19}, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "When performing feature extraction using Principal Component Analysis (PCA) on a dataset containing features like height, weight, age, gender, and blood pressure, the number of principal components to retain depends on various factors, including the dataset's size, the importance of capturing variance, and the desired dimensionality reduction.\n",
    "\n",
    "Here are some considerations for choosing the number of principal components to retain:\n",
    "\n",
    "1. **Explained Variance**:\n",
    "   - Check the explained variance ratio of each principal component. This indicates the proportion of the dataset's variance explained by each principal component.\n",
    "   - Retain enough principal components to capture a high percentage (e.g., 90% or more) of the total variance in the dataset.\n",
    "   - The cumulative explained variance plot can help determine the optimal number of principal components to retain.\n",
    "\n",
    "2. **Dimensionality Reduction**:\n",
    "   - Consider the level of dimensionality reduction required for the dataset.\n",
    "   - Retain a sufficient number of principal components to reduce the dimensionality while preserving as much information as possible.\n",
    "\n",
    "3. **Model Performance**:\n",
    "   - Evaluate the impact of retaining different numbers of principal components on the performance of downstream machine learning models.\n",
    "   - Conduct experiments with different numbers of principal components and assess how they affect model accuracy, stability, and computational efficiency.\n",
    "\n",
    "4. **Interpretability**:\n",
    "   - Consider the interpretability of the retained principal components and their relevance to the problem domain.\n",
    "   - Retain principal components that are easily interpretable and align with the underlying characteristics of the data.\n",
    "\n",
    "5. **Trade-off**:\n",
    "   - Find a balance between dimensionality reduction and information preservation.\n",
    "   - Avoid retaining too few principal components, as this may lead to information loss, or too many principal components, as this may introduce noise and overfitting.\n",
    "\n",
    "Without specific details about the dataset and its characteristics, it's challenging to determine the exact number of principal components to retain. However, as a general guideline, retaining principal components that capture a high percentage (e.g., 90% or more) of the total variance in the dataset while achieving a significant reduction in dimensionality is often desirable.\n",
    "\n",
    "After performing PCA on the dataset, you can analyze the explained variance ratio of each principal component and the cumulative explained variance to make an informed decision about how many principal components to retain. Experimentation and validation with downstream tasks can further refine the choice of the number of principal components to ensure optimal model performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
