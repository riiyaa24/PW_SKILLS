{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOPIC: Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Describe the purpose and benefits of pooling in CNN\n",
    "\n",
    "Pooling, or pooling layers, in Convolutional Neural Networks (CNNs) serve several important purposes and offer several benefits in the context of image processing and feature extraction. Here's an overview:\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "1. **Dimensionality Reduction**: Pooling reduces the spatial dimensions (width and height) of the feature maps produced by convolutional layers, which helps in reducing the computational complexity of the network. By reducing the number of parameters and computations, pooling makes the network more efficient.\n",
    "\n",
    "2. **Translation Invariance**: Pooling helps in creating feature maps that are more robust to small translations or distortions in the input image. By summarizing local information within pooling regions, pooling layers can make the network less sensitive to minor changes in the position of features in the input.\n",
    "\n",
    "3. **Feature Learning and Abstraction**: Pooling helps in capturing the most important features present in different regions of the input image. By aggregating information from neighboring pixels, pooling layers help in learning abstract and higher-level representations of the input.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "1. **Reduced Overfitting**: Pooling layers introduce a degree of spatial invariance and generalization, which can help in reducing overfitting. By summarizing local information, pooling layers prevent the network from focusing too much on specific details in the input data.\n",
    "\n",
    "2. **Computation Efficiency**: Pooling reduces the spatial dimensions of the feature maps, which leads to a reduction in the number of parameters and computations in subsequent layers of the network. This improves the efficiency of the network, making it faster to train and evaluate.\n",
    "\n",
    "3. **Improved Feature Extraction**: Pooling layers help in capturing the most salient features present in different regions of the input image. By summarizing local information, pooling layers highlight the most relevant features while discarding irrelevant or redundant information.\n",
    "\n",
    "4. **Robustness to Variations**: Pooling layers make CNNs more robust to spatial variations and distortions in the input data. By summarizing information within pooling regions, pooling layers help in creating feature maps that are invariant to small translations or distortions in the input image.\n",
    "\n",
    "Overall, pooling layers play a crucial role in CNNs by reducing the spatial dimensions of feature maps, introducing spatial invariance, and summarizing local information to extract salient features from the input data. They contribute to the efficiency, robustness, and generalization ability of CNNs in image processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain the difference between min pooling and max pooling\n",
    "\n",
    "Max pooling and min pooling are both types of pooling operations commonly used in Convolutional Neural Networks (CNNs) for dimensionality reduction and feature extraction. They differ primarily in how they aggregate information within pooling regions:\n",
    "\n",
    "1. **Max Pooling:**\n",
    "   - In max pooling, each pooling region (typically a small window) is assigned a single output value, which is the maximum value within that region.\n",
    "   - Max pooling is commonly used in CNNs to capture the most salient features present in different regions of the input.\n",
    "   - Max pooling emphasizes the presence of certain features by selecting the maximum value, effectively preserving spatial information about the most activated features.\n",
    "   - It helps in creating feature maps that are more robust to translation invariance, as it selects the most dominant feature in each region.\n",
    "   - Max pooling is often used in tasks where detecting specific features with high activation is crucial, such as object detection and recognition.\n",
    "\n",
    "2. **Min Pooling:**\n",
    "   - In min pooling, each pooling region is assigned a single output value, which is the minimum value within that region.\n",
    "   - Min pooling is less common compared to max pooling but can be useful in certain scenarios.\n",
    "   - Min pooling can be used to capture the least activated features within each region, which may be relevant for certain types of image processing tasks.\n",
    "   - It can help in creating feature maps that are more robust to variations in illumination or background noise, as it selects the least intense (minimum) value within each region.\n",
    "   - Min pooling might be used in tasks where identifying less intense features or suppressing noise is important.\n",
    "\n",
    "**Key Differences:**\n",
    "- Max pooling selects the maximum value within each pooling region, while min pooling selects the minimum value.\n",
    "- Max pooling emphasizes the most activated features, while min pooling emphasizes the least activated features.\n",
    "- Max pooling is more commonly used in CNN architectures and is often preferred for tasks such as image classification, object detection, and recognition.\n",
    "- Min pooling is less commonly used but can be beneficial in specific scenarios where detecting less intense features or suppressing noise is important.\n",
    "\n",
    "In summary, the main difference between max pooling and min pooling lies in the way they aggregate information within pooling regions, with max pooling focusing on the most activated features and min pooling focusing on the least activated features. Both pooling methods contribute to dimensionality reduction, translation invariance, and feature extraction in CNNs, but they may be suitable for different types of tasks and objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Discuss the concept of padding in CNN and its significance\n",
    "\n",
    "In Convolutional Neural Networks (CNNs), padding refers to the process of adding additional layers of zeros around the input image or feature maps before applying convolutional operations. Padding alters the spatial dimensions of the feature maps and plays a crucial role in controlling the size of the output feature maps after convolutional operations. There are two main types of padding:\n",
    "\n",
    "1. **Valid Padding (No Padding):** In valid padding, no padding is added to the input image or feature maps. As a result, the spatial dimensions of the output feature maps are reduced after convolution. With valid padding, the filter is only applied to positions where it fully overlaps with the input, and no padding is added around the input.\n",
    "\n",
    "2. **Same Padding:** In same padding, the necessary amount of padding is added to the input image or feature maps to ensure that the spatial dimensions of the output feature maps remain the same as the input. The amount of padding added is determined by the size of the convolutional filter (kernel size) and the stride used for the convolution operation.\n",
    "\n",
    "The significance of padding in CNNs includes:\n",
    "\n",
    "1. **Preservation of Spatial Information:** Padding helps in preserving the spatial dimensions of the input image or feature maps throughout the convolutional layers. This is particularly important in CNN architectures where spatial information is crucial for capturing patterns and features in the input data.\n",
    "\n",
    "2. **Prevention of Information Loss:** Valid padding can result in information loss at the boundaries of the input image or feature maps since the convolutional filter does not fully overlap with these regions. By adding padding, especially with same padding, information loss at the boundaries can be prevented, ensuring that all regions of the input are considered during convolution.\n",
    "\n",
    "3. **Control over Output Size:** Padding allows for greater control over the size of the output feature maps produced by convolutional operations. With same padding, the output feature maps have the same spatial dimensions as the input, facilitating the design of deeper networks and the stacking of multiple convolutional layers.\n",
    "\n",
    "4. **Stabilization of Convolutional Operations:** Padding helps in stabilizing the convolutional operations, especially for deep networks or when using larger filter sizes. It ensures that the convolutional filter has sufficient coverage of the input, reducing the risk of feature loss or degradation.\n",
    "\n",
    "Overall, padding is an essential technique in CNNs that helps in preserving spatial information, preventing information loss, controlling output size, and stabilizing convolutional operations. Properly chosen padding strategies can significantly impact the performance and effectiveness of CNN architectures in various computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compare and contrast zero-padding and valid padding in terms of their effects on the output feature map size\n",
    "\n",
    "Zero-padding and valid padding are two common padding techniques used in Convolutional Neural Networks (CNNs) that have different effects on the size of the output feature maps produced by convolutional operations. Here's a comparison of zero-padding and valid padding in terms of their effects on the output feature map size:\n",
    "\n",
    "1. **Zero-padding:**\n",
    "   - In zero-padding, additional layers of zeros are added around the input image or feature maps before applying convolutional operations.\n",
    "   - Zero-padding increases the spatial dimensions of the input by adding zeros around its boundaries.\n",
    "   - With zero-padding, the size of the output feature map can be controlled by adjusting the amount of padding added.\n",
    "   - Zero-padding ensures that the output feature map has the same spatial dimensions as the input, especially when using same padding.\n",
    "   - Example: If the input size is \\(n \\times n\\) and a \\(f \\times f\\) filter is applied with zero-padding of size \\(p\\), the output feature map size is \\((n + 2p - f + 1) \\times (n + 2p - f + 1)\\).\n",
    "\n",
    "2. **Valid padding:**\n",
    "   - In valid padding, no padding is added to the input image or feature maps before applying convolutional operations.\n",
    "   - Valid padding reduces the spatial dimensions of the input as the convolutional filter moves across the input, resulting in smaller output feature maps.\n",
    "   - With valid padding, the size of the output feature map is determined by the size of the input and the size of the convolutional filter.\n",
    "   - Valid padding may lead to information loss at the boundaries of the input, as the convolutional filter does not fully overlap with these regions.\n",
    "   - Example: If the input size is \\(n \\times n\\) and a \\(f \\times f\\) filter is applied with no padding, the output feature map size is \\((n - f + 1) \\times (n - f + 1)\\).\n",
    "\n",
    "**Comparison:**\n",
    "\n",
    "- **Effect on output size:** Zero-padding increases the output feature map size by adding zeros around the input, while valid padding reduces the output feature map size by not adding any padding.\n",
    "  \n",
    "- **Preservation of spatial information:** Zero-padding preserves spatial information by ensuring that the output feature map has the same spatial dimensions as the input, while valid padding may lead to information loss at the boundaries of the input.\n",
    "\n",
    "- **Control over output size:** Zero-padding provides greater control over the output feature map size, as the amount of padding can be adjusted, while valid padding results in smaller output feature maps determined solely by the size of the input and the convolutional filter.\n",
    "\n",
    "In summary, zero-padding and valid padding have contrasting effects on the output feature map size, with zero-padding increasing the size and valid padding reducing it. The choice between these padding techniques depends on the desired output size and the specific requirements of the CNN architecture being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOPIC: Exploring LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Provide a brief overview of LeNet-5 architecture\n",
    "\n",
    "LeNet-5 is a pioneering Convolutional Neural Network (CNN) architecture designed by Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner in 1998. It was one of the earliest CNN architectures and played a significant role in popularizing convolutional networks for computer vision tasks, particularly handwritten digit recognition. LeNet-5 consists of several layers, including convolutional layers, subsampling layers (pooling), and fully connected layers. Here's a brief overview of the LeNet-5 architecture:\n",
    "\n",
    "1. **Input Layer:** LeNet-5 takes as input grayscale images of size 32x32 pixels.\n",
    "\n",
    "2. **Convolutional Layers:** LeNet-5 has two convolutional layers:\n",
    "   - The first convolutional layer convolves the input image with a 5x5 kernel to produce feature maps.\n",
    "   - The second convolutional layer convolves the feature maps from the first layer with another 5x5 kernel to further extract higher-level features.\n",
    "\n",
    "3. **Subsampling (Pooling) Layers:** After each convolutional layer, LeNet-5 includes subsampling layers to reduce the spatial dimensions of the feature maps while preserving important information. These subsampling layers typically use average pooling or max pooling.\n",
    "\n",
    "4. **Fully Connected Layers:** Following the convolutional and subsampling layers, LeNet-5 has three fully connected layers:\n",
    "   - The first fully connected layer contains 120 neurons.\n",
    "   - The second fully connected layer contains 84 neurons.\n",
    "   - The final output layer contains 10 neurons, corresponding to the 10 possible classes (digits 0 through 9 in the case of handwritten digit recognition).\n",
    "\n",
    "5. **Activation Functions:** Throughout the network, LeNet-5 typically uses the tanh activation function for its neurons, although sigmoid activation functions were common at the time of its development.\n",
    "\n",
    "6. **Output Layer:** The output layer employs a softmax activation function to produce class probabilities, allowing LeNet-5 to perform multi-class classification.\n",
    "\n",
    "7. **Training:** LeNet-5 is trained using the backpropagation algorithm with stochastic gradient descent (SGD) optimization.\n",
    "\n",
    "Overall, LeNet-5 demonstrated the effectiveness of CNNs for handwritten digit recognition tasks and laid the groundwork for more advanced CNN architectures that followed. Its success paved the way for the development of modern deep learning techniques and architectures for a wide range of computer vision applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Describe the key components of LeNet-5 and their respective purposes\n",
    "\n",
    "LeNet-5, developed by Yann LeCun et al., is a seminal Convolutional Neural Network (CNN) architecture that played a significant role in the advancement of deep learning, particularly in the field of computer vision. The key components of LeNet-5 and their respective purposes are as follows:\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - The input layer accepts grayscale images of size 32x32 pixels.\n",
    "   - Purpose: It serves as the entry point for the input data into the network.\n",
    "\n",
    "2. **Convolutional Layers:**\n",
    "   - LeNet-5 includes two convolutional layers.\n",
    "   - The first convolutional layer convolves the input image with a 5x5 kernel to extract low-level features.\n",
    "   - The second convolutional layer convolves the feature maps from the first layer with another 5x5 kernel to capture higher-level features.\n",
    "   - Purpose: Convolutional layers extract spatial hierarchies of features from the input images, learning representations at different levels of abstraction.\n",
    "\n",
    "3. **Subsampling (Pooling) Layers:**\n",
    "   - Following each convolutional layer, LeNet-5 incorporates subsampling layers to reduce the spatial dimensions of the feature maps while preserving important information.\n",
    "   - These subsampling layers typically use average pooling or max pooling to downsample the feature maps.\n",
    "   - Purpose: Subsampling layers reduce computational complexity, improve translation invariance, and enhance the robustness of the network by summarizing local information.\n",
    "\n",
    "4. **Fully Connected Layers:**\n",
    "   - LeNet-5 comprises three fully connected layers.\n",
    "   - The first fully connected layer contains 120 neurons, followed by another fully connected layer with 84 neurons.\n",
    "   - The final output layer consists of 10 neurons, corresponding to the 10 possible classes (digits 0 through 9 in the case of handwritten digit recognition).\n",
    "   - Purpose: Fully connected layers perform high-level feature extraction and enable the network to learn complex non-linear relationships between features and classes.\n",
    "\n",
    "5. **Activation Functions:**\n",
    "   - Throughout the network, LeNet-5 typically uses the hyperbolic tangent (tanh) activation function for its neurons, although sigmoid activation functions were common at the time of its development.\n",
    "   - Purpose: Activation functions introduce non-linearity into the network, allowing it to learn complex mappings between inputs and outputs.\n",
    "\n",
    "6. **Output Layer:**\n",
    "   - The output layer employs a softmax activation function to produce class probabilities, enabling LeNet-5 to perform multi-class classification.\n",
    "   - Purpose: The output layer generates the final predictions of the network, indicating the probability of each input belonging to different classes.\n",
    "\n",
    "7. **Training Mechanism:**\n",
    "   - LeNet-5 is trained using the backpropagation algorithm with stochastic gradient descent (SGD) optimization.\n",
    "   - Purpose: The training mechanism updates the network's parameters iteratively to minimize a predefined loss function, thereby improving its performance on the given task.\n",
    "\n",
    "Overall, the key components of LeNet-5 work synergistically to extract hierarchical features from input images, classify them into different classes, and enable the network to learn discriminative representations for handwritten digit recognition and other image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks\n",
    "\n",
    "LeNet-5, being one of the pioneering Convolutional Neural Network (CNN) architectures, introduced several advancements in image classification tasks, particularly handwritten digit recognition. While it laid the groundwork for subsequent developments in deep learning, it also comes with its own set of advantages and limitations:\n",
    "\n",
    "**Advantages of LeNet-5:**\n",
    "\n",
    "1. **Effective Feature Extraction:** LeNet-5 effectively extracts hierarchical features from input images through its convolutional layers. These layers learn low-level features like edges and gradients in the initial layers and progressively more complex features in deeper layers, enabling the network to capture rich representations of the input data.\n",
    "\n",
    "2. **Translation Invariance:** By using subsampling (pooling) layers, LeNet-5 achieves translation invariance, making it robust to small shifts and distortions in the input images. This property is crucial for tasks where the position of objects within the image may vary.\n",
    "\n",
    "3. **Efficient Architecture:** LeNet-5's architecture strikes a balance between model complexity and computational efficiency. With relatively fewer parameters compared to modern CNNs, LeNet-5 is computationally efficient and can be trained on standard hardware.\n",
    "\n",
    "4. **Early Success in Handwritten Digit Recognition:** LeNet-5 demonstrated remarkable performance in handwritten digit recognition tasks, achieving high accuracy on benchmark datasets such as MNIST. Its success paved the way for the widespread adoption of CNNs in various image classification tasks.\n",
    "\n",
    "**Limitations of LeNet-5:**\n",
    "\n",
    "1. **Limited Model Capacity:** LeNet-5 has a relatively shallow architecture compared to modern CNNs, which limits its capacity to learn complex patterns and representations. As a result, it may struggle with more challenging image classification tasks that require deeper networks with more parameters.\n",
    "\n",
    "2. **Small Input Size:** LeNet-5 accepts input images of size 32x32 pixels, which may not be sufficient for tasks involving high-resolution images or fine-grained details. This limitation restricts its applicability to certain image classification tasks.\n",
    "\n",
    "3. **Lack of Non-Linearity:** LeNet-5 primarily uses hyperbolic tangent (tanh) activation functions, which may lead to the vanishing gradient problem and limit the network's ability to capture complex non-linear relationships in the data. Modern CNNs often use rectified linear unit (ReLU) activation functions to address this issue.\n",
    "\n",
    "4. **Limited Generalization:** While effective for handwritten digit recognition and similar tasks, LeNet-5 may not generalize well to more diverse datasets or real-world images with complex backgrounds and structures. Its performance may degrade when applied to tasks outside its original scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Implement LeNet-5 using a deep learning framework of your choice and train it on a publicly availabe dataset. Evaluate its performance and provide insights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:02<00:00, 3564027.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 333919.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 4087824.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = LeNet5().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/938], Loss: 0.3085\n",
      "Epoch [1/10], Step [200/938], Loss: 0.2787\n",
      "Epoch [1/10], Step [300/938], Loss: 0.1127\n",
      "Epoch [1/10], Step [400/938], Loss: 0.1520\n",
      "Epoch [1/10], Step [500/938], Loss: 0.0521\n",
      "Epoch [1/10], Step [600/938], Loss: 0.0386\n",
      "Epoch [1/10], Step [700/938], Loss: 0.0979\n",
      "Epoch [1/10], Step [800/938], Loss: 0.0479\n",
      "Epoch [1/10], Step [900/938], Loss: 0.1020\n",
      "Epoch [2/10], Step [100/938], Loss: 0.1908\n",
      "Epoch [2/10], Step [200/938], Loss: 0.0718\n",
      "Epoch [2/10], Step [300/938], Loss: 0.0089\n",
      "Epoch [2/10], Step [400/938], Loss: 0.0080\n",
      "Epoch [2/10], Step [500/938], Loss: 0.0365\n",
      "Epoch [2/10], Step [600/938], Loss: 0.0633\n",
      "Epoch [2/10], Step [700/938], Loss: 0.1901\n",
      "Epoch [2/10], Step [800/938], Loss: 0.0799\n",
      "Epoch [2/10], Step [900/938], Loss: 0.0775\n",
      "Epoch [3/10], Step [100/938], Loss: 0.0509\n",
      "Epoch [3/10], Step [200/938], Loss: 0.0422\n",
      "Epoch [3/10], Step [300/938], Loss: 0.0600\n",
      "Epoch [3/10], Step [400/938], Loss: 0.1346\n",
      "Epoch [3/10], Step [500/938], Loss: 0.0049\n",
      "Epoch [3/10], Step [600/938], Loss: 0.0096\n",
      "Epoch [3/10], Step [700/938], Loss: 0.0175\n",
      "Epoch [3/10], Step [800/938], Loss: 0.0209\n",
      "Epoch [3/10], Step [900/938], Loss: 0.0657\n",
      "Epoch [4/10], Step [100/938], Loss: 0.0363\n",
      "Epoch [4/10], Step [200/938], Loss: 0.0091\n",
      "Epoch [4/10], Step [300/938], Loss: 0.0100\n",
      "Epoch [4/10], Step [400/938], Loss: 0.0028\n",
      "Epoch [4/10], Step [500/938], Loss: 0.0082\n",
      "Epoch [4/10], Step [600/938], Loss: 0.0204\n",
      "Epoch [4/10], Step [700/938], Loss: 0.0230\n",
      "Epoch [4/10], Step [800/938], Loss: 0.0754\n",
      "Epoch [4/10], Step [900/938], Loss: 0.0998\n",
      "Epoch [5/10], Step [100/938], Loss: 0.0062\n",
      "Epoch [5/10], Step [200/938], Loss: 0.0029\n",
      "Epoch [5/10], Step [300/938], Loss: 0.0288\n",
      "Epoch [5/10], Step [400/938], Loss: 0.0639\n",
      "Epoch [5/10], Step [500/938], Loss: 0.0631\n",
      "Epoch [5/10], Step [600/938], Loss: 0.0071\n",
      "Epoch [5/10], Step [700/938], Loss: 0.0027\n",
      "Epoch [5/10], Step [800/938], Loss: 0.0205\n",
      "Epoch [5/10], Step [900/938], Loss: 0.0807\n",
      "Epoch [6/10], Step [100/938], Loss: 0.0102\n",
      "Epoch [6/10], Step [200/938], Loss: 0.0099\n",
      "Epoch [6/10], Step [300/938], Loss: 0.0021\n",
      "Epoch [6/10], Step [400/938], Loss: 0.0168\n",
      "Epoch [6/10], Step [500/938], Loss: 0.0104\n",
      "Epoch [6/10], Step [600/938], Loss: 0.0475\n",
      "Epoch [6/10], Step [700/938], Loss: 0.0019\n",
      "Epoch [6/10], Step [800/938], Loss: 0.0025\n",
      "Epoch [6/10], Step [900/938], Loss: 0.0023\n",
      "Epoch [7/10], Step [100/938], Loss: 0.0049\n",
      "Epoch [7/10], Step [200/938], Loss: 0.0020\n",
      "Epoch [7/10], Step [300/938], Loss: 0.0127\n",
      "Epoch [7/10], Step [400/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [500/938], Loss: 0.0017\n",
      "Epoch [7/10], Step [600/938], Loss: 0.0023\n",
      "Epoch [7/10], Step [700/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [800/938], Loss: 0.0973\n",
      "Epoch [7/10], Step [900/938], Loss: 0.0296\n",
      "Epoch [8/10], Step [100/938], Loss: 0.0334\n",
      "Epoch [8/10], Step [200/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [300/938], Loss: 0.0059\n",
      "Epoch [8/10], Step [400/938], Loss: 0.0011\n",
      "Epoch [8/10], Step [500/938], Loss: 0.0177\n",
      "Epoch [8/10], Step [600/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [700/938], Loss: 0.0990\n",
      "Epoch [8/10], Step [800/938], Loss: 0.0041\n",
      "Epoch [8/10], Step [900/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [100/938], Loss: 0.0292\n",
      "Epoch [9/10], Step [200/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [300/938], Loss: 0.0095\n",
      "Epoch [9/10], Step [400/938], Loss: 0.0094\n",
      "Epoch [9/10], Step [500/938], Loss: 0.0250\n",
      "Epoch [9/10], Step [600/938], Loss: 0.0046\n",
      "Epoch [9/10], Step [700/938], Loss: 0.0456\n",
      "Epoch [9/10], Step [800/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [900/938], Loss: 0.0274\n",
      "Epoch [10/10], Step [100/938], Loss: 0.0070\n",
      "Epoch [10/10], Step [200/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [300/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [400/938], Loss: 0.0029\n",
      "Epoch [10/10], Step [500/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [600/938], Loss: 0.0367\n",
      "Epoch [10/10], Step [700/938], Loss: 0.1103\n",
      "Epoch [10/10], Step [800/938], Loss: 0.0043\n",
      "Epoch [10/10], Step [900/938], Loss: 0.0005\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.65 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOPIC: Analyzing AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Present an overview of the alexnet architecture\n",
    "\n",
    "AlexNet is a landmark Convolutional Neural Network (CNN) architecture designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. It achieved a breakthrough in image classification performance by winning the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. Here's an overview of the AlexNet architecture:\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - AlexNet takes RGB images of size 224x224 pixels as input.\n",
    "\n",
    "2. **Convolutional Layers:**\n",
    "   - AlexNet consists of five convolutional layers, with each followed by a max-pooling layer.\n",
    "   - The first convolutional layer has 96 kernels of size 11x11 with a stride of 4 pixels.\n",
    "   - The subsequent convolutional layers have smaller kernel sizes (5x5) and vary in the number of output channels (256 and 384 channels).\n",
    "   - The convolutional layers use the rectified linear unit (ReLU) activation function.\n",
    "\n",
    "3. **Max Pooling Layers:**\n",
    "   - Following each convolutional layer, max-pooling layers with a kernel size of 3x3 and a stride of 2 pixels are applied.\n",
    "   - Max pooling helps in downsampling the feature maps, reducing spatial dimensions while preserving important features.\n",
    "\n",
    "4. **Normalization Layers:**\n",
    "   - AlexNet includes Local Response Normalization (LRN) layers after the first and second convolutional layers.\n",
    "   - LRN layers enhance the generalization capability of the network by normalizing the responses across different feature maps.\n",
    "\n",
    "5. **Fully Connected Layers:**\n",
    "   - After the convolutional and pooling layers, AlexNet includes three fully connected layers.\n",
    "   - The first two fully connected layers have 4096 neurons each, followed by a third fully connected layer with 1000 neurons corresponding to the number of classes in the ImageNet dataset.\n",
    "   - Dropout regularization is applied to the fully connected layers to prevent overfitting.\n",
    "\n",
    "6. **Softmax Layer:**\n",
    "   - The output layer employs a softmax activation function to produce class probabilities, enabling AlexNet to perform multi-class classification.\n",
    "\n",
    "7. **Training Mechanism:**\n",
    "   - AlexNet is trained using stochastic gradient descent (SGD) with momentum.\n",
    "   - Data augmentation techniques such as random cropping and horizontal flipping are used during training to increase the diversity of training samples.\n",
    "\n",
    "8. **Other Architectural Features:**\n",
    "   - AlexNet utilizes GPU acceleration to speed up training and inference.\n",
    "   - It introduced the concept of overlapping max-pooling, where the pooling regions overlap, providing a richer spatial hierarchy of features.\n",
    "\n",
    "AlexNet's success demonstrated the power of deep CNNs for image classification tasks and paved the way for subsequent advancements in deep learning and computer vision. Its architecture and training methodology have influenced many modern CNN architectures and techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain the architectural innovations introduced in AlexNet that contributed to its breakthrough performance\n",
    "\n",
    "AlexNet introduced several architectural innovations that significantly contributed to its breakthrough performance in image classification tasks. These innovations addressed key challenges in training deep convolutional neural networks (CNNs) and played a crucial role in improving the network's accuracy and efficiency. Here are the architectural innovations introduced in AlexNet:\n",
    "\n",
    "1. **Deep Convolutional Layers:**\n",
    "   - AlexNet was one of the first CNN architectures to incorporate multiple deep convolutional layers. It consists of five convolutional layers, which allowed the network to learn hierarchical representations of visual features at different levels of abstraction.\n",
    "   - Deeper architectures enable the network to capture more complex patterns and features in the input images, leading to improved classification performance.\n",
    "\n",
    "2. **ReLU Activation Function:**\n",
    "   - AlexNet replaced traditional activation functions like sigmoid or tanh with the rectified linear unit (ReLU) activation function in its convolutional layers.\n",
    "   - ReLU activation function accelerates the convergence of gradient-based optimization algorithms and helps alleviate the vanishing gradient problem by introducing non-linearity without saturating gradients.\n",
    "\n",
    "3. **Local Response Normalization (LRN):**\n",
    "   - AlexNet introduced Local Response Normalization (LRN) layers after the first and second convolutional layers.\n",
    "   - LRN layers help in normalizing the responses across different feature maps, enhancing the generalization capability of the network by promoting competition between neighboring features.\n",
    "\n",
    "4. **Overlapping Max Pooling:**\n",
    "   - AlexNet utilized overlapping max-pooling, where the pooling regions overlap, providing a richer spatial hierarchy of features compared to non-overlapping pooling.\n",
    "   - Overlapping pooling reduces the loss of spatial information and helps in capturing fine-grained features by allowing features to be shared across adjacent pooling regions.\n",
    "\n",
    "5. **Data Augmentation:**\n",
    "   - During training, AlexNet employed data augmentation techniques such as random cropping and horizontal flipping.\n",
    "   - Data augmentation increases the diversity of training samples, thereby improving the network's ability to generalize to unseen data and reducing overfitting.\n",
    "\n",
    "6. **Dropout Regularization:**\n",
    "   - AlexNet applied dropout regularization to the fully connected layers.\n",
    "   - Dropout randomly sets a fraction of the neurons in the fully connected layers to zero during training, preventing co-adaptation of neurons and reducing overfitting.\n",
    "\n",
    "7. **GPU Acceleration:**\n",
    "   - AlexNet utilized GPU acceleration to speed up training and inference.\n",
    "   - GPU acceleration significantly reduced the training time of deep neural networks, making it feasible to train large-scale models on large datasets.\n",
    "\n",
    "These architectural innovations collectively contributed to the breakthrough performance of AlexNet in image classification tasks, demonstrating the effectiveness of deep CNNs for computer vision applications. AlexNet's success laid the foundation for subsequent advancements in deep learning and inspired the development of more sophisticated CNN architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Discuss the role of convolutional layers, pooling layers and fully connected layers in AlexNet\n",
    "\n",
    "In AlexNet, convolutional layers, pooling layers, and fully connected layers play distinct yet complementary roles in the network architecture, contributing to its effectiveness in image classification tasks. Here's an overview of the roles of these layers in AlexNet:\n",
    "\n",
    "1. **Convolutional Layers:**\n",
    "   - AlexNet includes five convolutional layers, each followed by a ReLU activation function.\n",
    "   - Convolutional layers perform feature extraction by convolving input images with learnable filters (kernels), producing feature maps that capture spatial hierarchies of visual patterns.\n",
    "   - The first convolutional layer in AlexNet has a large kernel size (11x11) with a stride of 4 pixels, enabling it to capture low-level features such as edges and textures.\n",
    "   - Subsequent convolutional layers use smaller kernel sizes (5x5) to capture higher-level features and semantic information.\n",
    "   - Convolutional layers learn hierarchical representations of visual features through weight sharing and spatial locality, allowing the network to capture complex patterns and structures in the input images.\n",
    "\n",
    "2. **Pooling Layers:**\n",
    "   - After each convolutional layer, AlexNet employs max-pooling layers with a kernel size of 3x3 and a stride of 2 pixels.\n",
    "   - Pooling layers downsample the feature maps, reducing their spatial dimensions while retaining important features.\n",
    "   - Max-pooling is used to introduce translation invariance, making the network more robust to small spatial translations and distortions in the input images.\n",
    "   - Overlapping max-pooling in AlexNet helps in capturing fine-grained features by allowing features to be shared across adjacent pooling regions.\n",
    "\n",
    "3. **Fully Connected Layers:**\n",
    "   - AlexNet includes three fully connected layers after the convolutional and pooling layers.\n",
    "   - The fully connected layers perform high-level feature extraction and enable the network to learn complex non-linear mappings between features and classes.\n",
    "   - The first two fully connected layers have 4096 neurons each, followed by a third fully connected layer with 1000 neurons corresponding to the number of classes in the ImageNet dataset.\n",
    "   - Dropout regularization is applied to the fully connected layers to prevent overfitting by randomly dropping a fraction of neurons during training.\n",
    "   - The final fully connected layer employs a softmax activation function to produce class probabilities, enabling AlexNet to perform multi-class classification.\n",
    "\n",
    "In summary, convolutional layers extract hierarchical representations of visual features, pooling layers downsample feature maps while preserving important information, and fully connected layers perform high-level feature extraction and classification. Together, these layers enable AlexNet to learn discriminative representations of input images and achieve state-of-the-art performance in image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Implement AlexNet using a deep learning framework of your choice and evaluate its performance on a dataset of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">34,944</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">614,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">885,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,327,488</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">884,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">16,781,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,970</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m96\u001b[0m)       │        \u001b[38;5;34m34,944\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_9 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m614,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_10 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m0\u001b[0m, \u001b[38;5;34m0\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m0\u001b[0m, \u001b[38;5;34m0\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │       \u001b[38;5;34m885,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m0\u001b[0m, \u001b[38;5;34m0\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │     \u001b[38;5;34m1,327,488\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m0\u001b[0m, \u001b[38;5;34m0\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m884,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_11 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m0\u001b[0m, \u001b[38;5;34m0\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m0\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │    \u001b[38;5;34m16,781,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m40,970\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,573,578</span> (78.48 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,573,578\u001b[0m (78.48 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,573,578</span> (78.48 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,573,578\u001b[0m (78.48 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling MaxPooling2D.call().\n\n\u001b[1mNegative dimension size caused by subtracting 3 from 2 for '{{node sequential_3_1/max_pooling2d_10_1/MaxPool2d}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](sequential_3_1/conv2d_16_1/Relu)' with input shapes: [?,2,2,256].\u001b[0m\n\nArguments received by MaxPooling2D.call():\n  • inputs=tf.Tensor(shape=(None, 2, 2, 256), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[0;32m     50\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Smita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Smita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling MaxPooling2D.call().\n\n\u001b[1mNegative dimension size caused by subtracting 3 from 2 for '{{node sequential_3_1/max_pooling2d_10_1/MaxPool2d}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](sequential_3_1/conv2d_16_1/Relu)' with input shapes: [?,2,2,256].\u001b[0m\n\nArguments received by MaxPooling2D.call():\n  • inputs=tf.Tensor(shape=(None, 2, 2, 256), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Define AlexNet architecture\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((3, 3), strides=(2, 2)),\n",
    "    layers.Conv2D(256, (5, 5), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((3, 3), strides=(2, 2)),\n",
    "    layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
    "    layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
    "    layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
