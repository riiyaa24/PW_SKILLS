{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Gradient Boosting Regression, often referred to as Gradient Boosting Machine (GBM), is a powerful machine learning technique used for regression tasks. It is an ensemble learning method that combines the predictions of multiple weak learners (typically decision trees) to create a strong regression model. \n",
    "\n",
    "In Gradient Boosting Regression, the model is built in a sequential manner, where each new weak learner is trained to correct the errors made by the previous ones. The key idea behind Gradient Boosting Regression is to minimize a loss function (typically the mean squared error) by iteratively fitting weak learners to the residuals of the current ensemble model.\n",
    "\n",
    "Here's how Gradient Boosting Regression works:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Initialize the ensemble model with a simple model, often the mean or median of the target variable.\n",
    "\n",
    "2. **Sequential Training**:\n",
    "   - Train a weak learner (e.g., decision tree) to predict the residuals (the differences between the true target values and the predictions of the current ensemble model).\n",
    "   - Compute the negative gradient of the loss function with respect to the predictions of the current ensemble model. This gradient represents the direction in which the loss function decreases most rapidly.\n",
    "\n",
    "3. **Shrinkage**:\n",
    "   - Introduce a shrinkage parameter (learning rate) to control the contribution of each weak learner to the ensemble. This parameter scales the predictions of each weak learner before adding them to the ensemble.\n",
    "\n",
    "4. **Additive Update**:\n",
    "   - Update the current ensemble model by adding the predictions of the newly trained weak learner, scaled by the shrinkage parameter.\n",
    "   - The ensemble model is updated in an additive manner, with each new weak learner focusing on the residuals of the previous ensemble model.\n",
    "\n",
    "5. **Iterative Process**:\n",
    "   - Repeat steps 2 to 4 for a predefined number of iterations or until a stopping criterion is met. Each iteration improves the overall model by focusing on the residuals of the previous ensemble model.\n",
    "\n",
    "6. **Final Prediction**:\n",
    "   - Combine the predictions of all weak learners in the ensemble to make the final prediction for regression.\n",
    "   - For regression tasks, the final prediction is often the sum of the predictions of all weak learners, possibly scaled by the shrinkage parameter.\n",
    "\n",
    "Gradient Boosting Regression is known for its high predictive accuracy and robustness, especially when used with weak learners like decision trees. It is widely used in various regression tasks, including predicting house prices, stock prices, and customer lifetime value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 31.735482349161565\n",
      "R-squared: 0.9772379183627112\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize predictions with the mean of target variable\n",
    "        predictions = np.mean(y) * np.ones_like(y)\n",
    "\n",
    "        # Iteratively train weak learners and update predictions\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate residuals\n",
    "            residuals = y - predictions\n",
    "\n",
    "            # Train weak learner (decision tree) on residuals\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            model.fit(X, residuals)\n",
    "\n",
    "            # Update predictions using the weak learner\n",
    "            update = self.learning_rate * model.predict(X)\n",
    "            predictions += update\n",
    "\n",
    "            # Store the weak learner and its weight\n",
    "            self.models.append(model)\n",
    "            self.weights.append(self.learning_rate)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using all weak learners\n",
    "        predictions = np.sum([model.predict(X) * weight for model, weight in zip(self.models, self.weights)], axis=0)\n",
    "        return predictions\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    y_mean = np.mean(y_true)\n",
    "    SS_res = np.sum((y_true - y_pred) ** 2)\n",
    "    SS_tot = np.sum((y_true - y_mean) ** 2)\n",
    "    return 1 - (SS_res / SS_tot)\n",
    "\n",
    "# Example usage\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Gradient Boosting Regression model\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r_squared(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 150}\n",
      "Mean Squared Error (Best Model): 32.456435181317\n",
      "R-squared (Best Model): 0.9767208193301015\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "class CustomGradientBoostingRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize predictions with the mean of target variable\n",
    "        predictions = np.mean(y) * np.ones_like(y)\n",
    "\n",
    "        # Iteratively train weak learners and update predictions\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate residuals\n",
    "            residuals = y - predictions\n",
    "\n",
    "            # Train weak learner (decision tree) on residuals\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            model.fit(X, residuals)\n",
    "\n",
    "            # Update predictions using the weak learner\n",
    "            update = self.learning_rate * model.predict(X)\n",
    "            predictions += update\n",
    "\n",
    "            # Store the weak learner and its weight\n",
    "            self.models.append(model)\n",
    "            self.weights.append(self.learning_rate)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using all weak learners\n",
    "        predictions = np.sum([model.predict(X) * weight for model, weight in zip(self.models, self.weights)], axis=0)\n",
    "        return predictions\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        # Calculate R-squared score\n",
    "        return r_squared(y, self.predict(X))\n",
    "\n",
    "# Example usage with GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Initialize Custom Gradient Boosting Regressor\n",
    "gb_regressor = CustomGradientBoostingRegressor()\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Initialize Custom Gradient Boosting Regressor with best parameters\n",
    "best_gb_regressor = CustomGradientBoostingRegressor(**best_params)\n",
    "\n",
    "# Fit model with best parameters\n",
    "best_gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r_squared(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (Best Model):\", mse)\n",
    "print(\"R-squared (Best Model):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "In Gradient Boosting, a weak learner refers to a simple or base model that performs slightly better than random guessing on a given task. Weak learners are typically simple models with limited predictive power, such as decision trees with shallow depth (often called decision stumps) or linear models.\n",
    "\n",
    "The concept of weak learners is central to Gradient Boosting algorithms, as they are combined sequentially to form a strong learner (the ensemble model). Each weak learner contributes a small improvement to the overall model, focusing on capturing different aspects of the data or correcting errors made by previous weak learners.\n",
    "\n",
    "The key characteristics of a weak learner in Gradient Boosting are:\n",
    "\n",
    "1. **Limited Complexity**: Weak learners are deliberately kept simple to prevent overfitting and improve generalization performance. They often have limited expressive power, such as shallow decision trees with few nodes or linear models with few parameters.\n",
    "\n",
    "2. **Better Than Random Guessing**: While weak learners may not be highly accurate on their own, they perform slightly better than random guessing on the task. This means that they have some ability to capture patterns or relationships in the data, albeit weakly.\n",
    "\n",
    "3. **Sequential Combination**: Weak learners are combined sequentially in Gradient Boosting algorithms. Each new weak learner is trained to correct the errors made by the ensemble of previous weak learners. This iterative process gradually improves the model's performance over multiple iterations.\n",
    "\n",
    "4. **Complementary Predictions**: Weak learners typically focus on capturing different aspects of the data or correcting different types of errors. By combining multiple weak learners with complementary strengths, Gradient Boosting algorithms can build a strong ensemble model that achieves high predictive accuracy.\n",
    "\n",
    "Overall, weak learners play a crucial role in Gradient Boosting by contributing simple but effective models that, when combined sequentially, result in a powerful ensemble model capable of achieving high accuracy on a variety of machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm stems from the idea of building a strong learner (a highly accurate predictive model) by sequentially combining multiple weak learners (models that perform slightly better than random guessing) to correct errors made by the ensemble of previous learners. Here's a high-level overview of the intuition behind Gradient Boosting:\n",
    "\n",
    "1. **Additive Modeling**:\n",
    "   - Gradient Boosting builds the final model in an additive manner, adding weak learners sequentially to improve predictive accuracy.\n",
    "   - Each new weak learner is trained to capture the errors (residuals) made by the ensemble of previous weak learners.\n",
    "\n",
    "2. **Gradient Descent**:\n",
    "   - The algorithm minimizes a loss function (such as mean squared error for regression or log-loss for classification) by iteratively updating the model's predictions in the direction of the negative gradient of the loss function.\n",
    "   - At each iteration, Gradient Boosting calculates the negative gradient of the loss function with respect to the current ensemble's predictions. This gradient indicates the direction in which the loss function decreases most rapidly.\n",
    "\n",
    "3. **Sequential Correction of Errors**:\n",
    "   - Gradient Boosting focuses on minimizing the residuals (errors) of the current ensemble model. Each new weak learner is trained to capture the residuals of the previous ensemble model, effectively correcting errors made by the ensemble.\n",
    "\n",
    "4. **Shrinkage**:\n",
    "   - To control the contribution of each weak learner to the ensemble, Gradient Boosting introduces a shrinkage parameter (learning rate). This parameter scales the predictions of each weak learner before adding them to the ensemble.\n",
    "   - A smaller learning rate leads to slower learning but often results in better generalization and robustness.\n",
    "\n",
    "5. **Combinatorial Strength**:\n",
    "   - By combining multiple weak learners sequentially, Gradient Boosting builds a strong ensemble model that captures complex relationships in the data.\n",
    "   - Each weak learner contributes a small improvement to the model, and their collective strength leads to significant gains in predictive accuracy.\n",
    "\n",
    "6. **Robustness to Overfitting**:\n",
    "   - Gradient Boosting is inherently resistant to overfitting due to its additive nature and the use of weak learners.\n",
    "   - The algorithm focuses on minimizing the residuals of the current ensemble model, rather than fitting the training data too closely.\n",
    "\n",
    "Overall, the intuition behind Gradient Boosting lies in iteratively improving the model by sequentially adding weak learners to correct errors made by the ensemble. This iterative process leads to a powerful ensemble model capable of achieving high predictive accuracy and robustness on a variety of machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner, with each new weak learner added to correct the errors made by the ensemble of previous weak learners. Here's how the Gradient Boosting algorithm builds the ensemble of weak learners:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - The algorithm starts by initializing the ensemble with a simple model, typically the mean of the target variable for regression tasks or a constant for classification tasks.\n",
    "\n",
    "2. **Sequential Training**:\n",
    "   - At each iteration, a new weak learner (usually a decision tree with limited depth) is trained on the residuals (errors) of the current ensemble model.\n",
    "   - The residuals represent the difference between the true target values and the predictions of the current ensemble model.\n",
    "   - The weak learner is trained to capture these residuals and improve the predictions of the ensemble.\n",
    "\n",
    "3. **Gradient Calculation**:\n",
    "   - After training the weak learner, the algorithm calculates the negative gradient of the loss function with respect to the current ensemble's predictions.\n",
    "   - This gradient indicates the direction in which the loss function decreases most rapidly.\n",
    "   - The negative gradient represents the residuals that need to be corrected by the new weak learner.\n",
    "\n",
    "4. **Shrinkage**:\n",
    "   - To control the contribution of each weak learner to the ensemble, the algorithm introduces a shrinkage parameter (learning rate).\n",
    "   - The predictions of the new weak learner are scaled by the learning rate before being added to the ensemble.\n",
    "\n",
    "5. **Additive Update**:\n",
    "   - The predictions of the new weak learner are added to the predictions of the current ensemble, resulting in an updated ensemble model.\n",
    "   - The ensemble model is updated in an additive manner, with each new weak learner contributing a small improvement to the overall predictions.\n",
    "\n",
    "6. **Iteration**:\n",
    "   - Steps 2 to 5 are repeated for a predefined number of iterations or until a stopping criterion is met.\n",
    "   - Each iteration focuses on minimizing the residuals of the current ensemble model by sequentially adding weak learners to correct errors made by the ensemble.\n",
    "\n",
    "7. **Final Ensemble**:\n",
    "   - The final ensemble model consists of the predictions of all weak learners added together, scaled by their respective learning rates.\n",
    "   - The ensemble model captures complex relationships in the data by combining the predictions of multiple weak learners in a sequential manner.\n",
    "\n",
    "By iteratively training weak learners on the residuals of the current ensemble model and adding them to the ensemble, the Gradient Boosting algorithm builds a strong ensemble model capable of achieving high predictive accuracy on a variety of machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves several steps that build upon each other. Below are the key steps involved in understanding the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "1. **Loss Function**:\n",
    "   - Start by defining a loss function that measures the difference between the model's predictions and the true labels. Common loss functions include mean squared error (MSE) for regression problems and logistic loss for binary classification problems.\n",
    "\n",
    "2. **Gradient Descent**:\n",
    "   - Understand the concept of gradient descent, a optimization technique used to minimize the loss function by iteratively updating the model parameters in the direction of the negative gradient of the loss function.\n",
    "\n",
    "3. **Gradient Boosting as Additive Modeling**:\n",
    "   - Realize that Gradient Boosting builds the final model in an additive manner, by sequentially adding weak learners (usually decision trees) to minimize the loss function.\n",
    "\n",
    "4. **Residuals**:\n",
    "   - Recognize that at each stage of Gradient Boosting, the weak learner is trained on the residuals (the differences between the true labels and the model predictions) of the current ensemble model.\n",
    "\n",
    "5. **Gradient of the Loss Function**:\n",
    "   - Compute the negative gradient of the loss function with respect to the model's predictions. This represents the direction in which the loss function decreases most rapidly.\n",
    "\n",
    "6. **Weak Learner Training**:\n",
    "   - Train a weak learner (e.g., decision tree) to predict the negative gradient of the loss function, effectively fitting a model to the residuals of the current ensemble model.\n",
    "\n",
    "7. **Shrinkage**:\n",
    "   - Introduce a shrinkage parameter (learning rate) to control the contribution of each weak learner to the ensemble. This parameter scales the predictions of each weak learner before adding them to the ensemble.\n",
    "\n",
    "8. **Additive Update**:\n",
    "   - Update the current ensemble model by adding the predictions of the newly trained weak learner, scaled by the shrinkage parameter.\n",
    "\n",
    "9. **Iterative Process**:\n",
    "   - Repeat steps 4 to 8 for a predefined number of iterations or until a stopping criterion is met. Each iteration improves the overall model by focusing on the residuals of the previous ensemble model.\n",
    "\n",
    "10. **Final Prediction**:\n",
    "    - Combine the predictions of all weak learners in the ensemble to make the final prediction. For regression, this may involve summing the predictions; for classification, it may involve using a weighted vote.\n",
    "\n",
    "By understanding these steps, one can grasp the mathematical intuition behind Gradient Boosting and how it iteratively improves the model by focusing on the residuals of the previous iterations. This understanding forms the basis for implementing and tuning Gradient Boosting algorithms effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
