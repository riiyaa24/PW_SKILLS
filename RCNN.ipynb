{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the objectives of using selective search in R-CNN?\n",
    "\n",
    "Selective search is a region proposal algorithm commonly used in object detection frameworks like R-CNN (Region-based Convolutional Neural Network). Its objectives include:\n",
    "\n",
    "1. **Generate Region Proposals**: Selective search generates a diverse set of region proposals that are likely to contain objects. These proposals serve as potential regions where objects might be located within an image.\n",
    "\n",
    "2. **Reduce Computation**: By providing a smaller set of region proposals compared to exhaustive sliding window approaches, selective search helps reduce the computational burden of object detection algorithms like R-CNN.\n",
    "\n",
    "3. **Improve Accuracy**: The diverse nature of region proposals generated by selective search helps in capturing objects of different sizes, shapes, and appearances, thereby potentially improving the accuracy of object localization and recognition.\n",
    "\n",
    "4. **Enable Spatial Pyramid Pooling**: Selective search outputs can be used for spatial pyramid pooling, which allows the network to handle objects at different scales effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain the following phases involved in R-CNN\n",
    "a. Region proposal\n",
    "Purpose: This phase involves generating potential regions within an image that might contain objects. Techniques like selective search are used to propose these regions.\n",
    "Output: The algorithm outputs a set of bounding boxes (region proposals) around potential objects in the image.\n",
    "\n",
    "b. warping and resizing\n",
    "Purpose: Each region proposal from the previous phase is cropped from the image and warped/resized to a fixed size.\n",
    "Normalization: Ensures that each region proposal has the same dimensions, which is necessary for consistent input into subsequent stages of the model.\n",
    "\n",
    "c. pre trained CNN architecture\n",
    "Purpose: A pre-trained Convolutional Neural Network (CNN), like VGG, ResNet, or similar, is used as a feature extractor.\n",
    "Feature Extraction: The fixed-size region proposals are fed into the CNN, and the network extracts high-level features from each region.\n",
    "Transfer Learning: By using a pre-trained CNN, the model leverages features learned from a large dataset, which improves the performance of object detection.\n",
    "\n",
    "d. pre trained SVM models\n",
    "Purpose: Support Vector Machine (SVM) classifiers are trained on top of the features extracted by the CNN.\n",
    "Classification: Each region proposal is classified into different classes (object categories) based on the features extracted by the CNN.\n",
    "Fine-tuning: SVMs are fine-tuned to improve the accuracy of object classification.\n",
    "\n",
    "e. clean up\n",
    "Purpose: After classification, post-processing steps are applied to refine the results.\n",
    "Non-Maximum Suppression: Eliminates overlapping bounding boxes and retains only the most confident ones.\n",
    "Thresholding: Filters out regions with low confidence scores to improve overall accuracy.\n",
    "\n",
    "f. implementation of bounding box\n",
    "Purpose: Once objects are classified and localized, bounding boxes are drawn around detected objects.\n",
    "Visualization: Bounding boxes mark the boundaries of detected objects, aiding in visual interpretation and further analysis of the detected objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What are the possible pre-trained CNNs we can use in pre-trained CNN architecture?\n",
    "\n",
    "In the context of R-CNN, various pre-trained CNN architectures can be used for feature extraction. Some of the commonly used pre-trained CNNs include:\n",
    "\n",
    "AlexNet:\n",
    "One of the first deep convolutional networks to achieve significant performance improvements in image classification tasks.\n",
    "\n",
    "VGGNet (VGG16, VGG19):\n",
    "Known for its simplicity and effectiveness, VGGNet has deep architectures with 16 and 19 layers, respectively.\n",
    "\n",
    "GoogLeNet (Inception):\n",
    "Introduces the Inception module, which allows for more efficient computation by varying the kernel sizes within the network.\n",
    "\n",
    "ResNet (Residual Networks):\n",
    "Uses residual connections to enable training of very deep networks (e.g., ResNet-50, ResNet-101, ResNet-152).\n",
    "\n",
    "DenseNet:\n",
    "Utilizes dense connections between layers, promoting feature reuse and improving gradient flow.\n",
    "\n",
    "MobileNet:\n",
    "Designed for efficient computation, making it suitable for mobile and embedded vision applications.\n",
    "\n",
    "EfficientNet:\n",
    "Balances network depth, width, and resolution to achieve state-of-the-art performance with fewer parameters.\n",
    "\n",
    "Inception-ResNet:\n",
    "Combines the strengths of Inception and ResNet architectures for improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How is SVM implemented in the R-CNN framework?\n",
    "\n",
    "In the R-CNN framework, SVM (Support Vector Machine) classifiers are used for object classification based on features extracted by the pre-trained CNN. Here's a step-by-step explanation of how SVM is implemented:\n",
    "\n",
    "Feature Extraction:\n",
    "\n",
    "Each region proposal is resized to a fixed size and passed through the pre-trained CNN.\n",
    "The CNN extracts high-level features from the region proposal, typically from the fully connected layers.\n",
    "Training SVM:\n",
    "\n",
    "Dataset Preparation: For each region proposal, features extracted by the CNN are stored, along with the corresponding class labels.\n",
    "SVM Training: Separate SVM classifiers are trained for each object class using the extracted features. The goal is to distinguish each class from all other classes (one-vs-all classification).\n",
    "Positive and Negative Samples: During training, positive samples (regions containing the object) and negative samples (regions without the object) are used to train the SVMs.\n",
    "Inference:\n",
    "\n",
    "Feature Extraction: For each new image, region proposals are generated and features are extracted using the pre-trained CNN.\n",
    "Classification: The extracted features are fed into the trained SVM classifiers.\n",
    "Score Calculation: Each SVM classifier provides a confidence score for the presence of its corresponding object class in each region proposal.\n",
    "Post-processing:\n",
    "\n",
    "Non-Maximum Suppression (NMS): After classification, overlapping bounding boxes are processed to retain the most confident detections and eliminate redundant ones.\n",
    "Thresholding: Regions with confidence scores below a certain threshold are discarded to reduce false positives.\n",
    "By using SVM classifiers in conjunction with a pre-trained CNN, R-CNN achieves effective object classification and localization, leveraging the strengths of both CNNs for feature extraction and SVMs for robust classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How does non-maximum suppression work?\n",
    "\n",
    "Non-Maximum Suppression (NMS) is a technique used in object detection to eliminate redundant bounding boxes that refer to the same object and retain only the most confident one. Here's how it works step-by-step:\n",
    "\n",
    "### Steps of Non-Maximum Suppression (NMS):\n",
    "\n",
    "1. **Score Sorting**:\n",
    "   - First, all the bounding boxes generated for a particular class are sorted based on their confidence scores (i.e., the probability or confidence that they contain the object).\n",
    "\n",
    "2. **Selection**:\n",
    "   - The bounding box with the highest confidence score is selected as the first bounding box.\n",
    "\n",
    "3. **Overlap Calculation**:\n",
    "   - For the selected bounding box, the overlap (Intersection over Union, IoU) is calculated with all other remaining bounding boxes.\n",
    "\n",
    "4. **Suppression**:\n",
    "   - Any bounding box that has an IoU greater than a specified threshold (e.g., 0.5) with the selected bounding box is suppressed (i.e., removed from consideration) because it is considered redundant.\n",
    "\n",
    "5. **Iteration**:\n",
    "   - The process is repeated with the next highest confidence bounding box in the list, recalculating the IoU with the remaining boxes, and suppressing those with high overlap.\n",
    "   - This iteration continues until all bounding boxes have been processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How Fast R-CNN is better than R-CNN?\n",
    "\n",
    "Fast R-CNN offers several improvements over the original R-CNN, addressing its main limitations and enhancing performance and efficiency. Here are the key differences and advantages of Fast R-CNN compared to R-CNN:\n",
    "\n",
    "### 1. **Training and Inference Speed**:\n",
    "- **R-CNN**: Involves three separate stages: region proposal, feature extraction, and classification. Each region proposal is processed independently, resulting in a high computational cost and slow performance.\n",
    "- **Fast R-CNN**: Integrates feature extraction and region classification into a single network. It processes the entire image through a convolutional network once to produce a feature map, and then applies region proposals to this feature map. This reduces redundant computations, significantly speeding up both training and inference.\n",
    "\n",
    "### 2. **Single-Stage Training**:\n",
    "- **R-CNN**: Requires a multi-stage pipeline involving separate training for the CNN, SVM classifiers, and bounding box regressors.\n",
    "- **Fast R-CNN**: Uses a single-stage training process where the entire network, including region classification and bounding box regression, is trained end-to-end with a multi-task loss function.\n",
    "\n",
    "### 3. **Memory Efficiency**:\n",
    "- **R-CNN**: Stores each region proposal as a separate image, leading to high memory consumption.\n",
    "- **Fast R-CNN**: Utilizes the feature map of the entire image, extracting features from the regions of interest (RoIs) directly from this shared feature map. This approach is much more memory-efficient.\n",
    "\n",
    "### 4. **Region of Interest (RoI) Pooling**:\n",
    "- **R-CNN**: Uses a fixed-size warping of each region proposal before feature extraction, which can be computationally expensive and lossy.\n",
    "- **Fast R-CNN**: Introduces RoI Pooling, which extracts fixed-size feature maps from the regions of interest directly from the shared feature map. RoI Pooling preserves spatial information and is more computationally efficient.\n",
    "\n",
    "### 5. **Bounding Box Regression**:\n",
    "- **R-CNN**: Applies bounding box regression in a separate post-processing step after classification.\n",
    "- **Fast R-CNN**: Integrates bounding box regression into the network, allowing it to be trained jointly with the classification task. This results in better localization accuracy.\n",
    "\n",
    "### 6. **Handling of Overlapping RoIs**:\n",
    "- **R-CNN**: Processes each region proposal independently, which can result in redundant computations for overlapping regions.\n",
    "- **Fast R-CNN**: By using a single feature map and applying RoI Pooling, overlapping regions share computations, reducing redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using mathematical intution, explains ROI pooling in fast R-CNN\n",
    "\n",
    "RoI (Region of Interest) Pooling in Fast R-CNN is a critical operation that allows the network to handle variable-sized regions of interest and produce fixed-size feature maps for each region. Here's a mathematical intuition behind RoI Pooling:\n",
    "\n",
    "### Steps of RoI Pooling:\n",
    "\n",
    "1. **Feature Map Generation**:\n",
    "   - Given an input image, a convolutional neural network (CNN) processes it to produce a feature map.\n",
    "   - Let’s denote this feature map as \\( F \\) with dimensions \\( H \\times W \\times C \\) where \\( H \\) is height, \\( W \\) is width, and \\( C \\) is the number of channels.\n",
    "\n",
    "2. **Region Proposals**:\n",
    "   - Region proposals (bounding boxes) are generated, which might have different sizes.\n",
    "   - Suppose we have a region proposal with coordinates \\((x_1, y_1, x_2, y_2)\\), defining the top-left and bottom-right corners of the bounding box.\n",
    "\n",
    "3. **Spatial Binning**:\n",
    "   - The region proposal is divided into a fixed number of bins (e.g., \\( k \\times k \\) bins). This fixed size is typically \\( 7 \\times 7 \\) or \\( 14 \\times 14 \\), depending on the architecture.\n",
    "   - Each bin will correspond to a \\( h' \\times w' \\) sub-region of the feature map, where \\( h' \\) and \\( w' \\) are the heights and widths of the sub-regions.\n",
    "\n",
    "   Mathematically, if the region proposal has width \\( w = x_2 - x_1 \\) and height \\( h = y_2 - y_1 \\), the dimensions of each bin are:\n",
    "   \\[\n",
    "   w' = \\frac{w}{k}, \\quad h' = \\frac{h}{k}\n",
    "   \\]\n",
    "\n",
    "4. **Max Pooling Within Each Bin**:\n",
    "   - For each bin, max pooling is applied to extract the most prominent feature within the bin.\n",
    "   - If a bin spans \\( w' \\times h' \\) pixels in the feature map, the max pooling operation identifies the maximum value among these pixels for each channel.\n",
    "\n",
    "   Mathematically, for each bin, the max pooling operation can be represented as:\n",
    "   \\[\n",
    "   \\text{bin}(i, j) = \\max_{x_1 + i \\cdot w' \\leq x < x_1 + (i+1) \\cdot w'} \\max_{y_1 + j \\cdot h' \\leq y < y_1 + (j+1) \\cdot h'} F(x, y)\n",
    "   \\]\n",
    "   where \\( \\text{bin}(i, j) \\) is the value of the bin at position \\( (i, j) \\) in the \\( k \\times k \\) grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Explain following processes:\n",
    "a. ROI Projection : ROI Projection is the process of mapping the coordinates of the region proposals (or Regions of Interest, RoIs) from the original image space to the corresponding coordinates in the feature map space produced by the convolutional layers of a neural network.\n",
    "b. ROI Pooling : ROI Pooling is the process of extracting a fixed-size feature map (e.g.,7×7) for each region proposal from the feature map, regardless of the original size of the region proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. In comparision with R-CNN, why did the object classifier activation function change in Fast R-CNN?\n",
    "\n",
    "In Fast R-CNN, the object classifier activation function and overall architecture changes were made to improve efficiency, accuracy, and simplicity compared to the original R-CNN. Here are the key reasons for the changes:\n",
    "\n",
    "### 1. Integration of Classification and Regression:\n",
    "\n",
    "- **R-CNN**: In the original R-CNN, object classification was performed using linear SVM classifiers. The CNN was used only for feature extraction, and the classification was done separately using SVMs for each object class.\n",
    "- **Fast R-CNN**: Fast R-CNN integrates both object classification and bounding box regression into a single network. This end-to-end approach allows for simultaneous optimization of both tasks, improving overall performance and accuracy.\n",
    "\n",
    "### 2. Use of Softmax for Classification:\n",
    "\n",
    "- **R-CNN**: R-CNN used a set of linear SVMs for object classification. This required separate training stages for the CNN and the SVMs, leading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. What major changes in Faster R-CNN compared to Fast R-CNN?\n",
    "\n",
    "Faster R-CNN introduced significant improvements over Fast R-CNN by addressing the inefficiencies related to region proposal generation. Here are the major changes and enhancements:\n",
    "\n",
    "### 1. **Region Proposal Network (RPN)**:\n",
    "\n",
    "- **Fast R-CNN**: Uses external region proposal methods like Selective Search to generate region proposals. This step is computationally expensive and not integrated into the neural network, leading to slower performance.\n",
    "- **Faster R-CNN**: Introduces a Region Proposal Network (RPN) that generates region proposals directly within the network. RPN is a fully convolutional network that shares convolutional layers with the object detection network, making the proposal generation both faster and more efficient.\n",
    "\n",
    "### 2. **End-to-End Training**:\n",
    "\n",
    "- **Fast R-CNN**: The region proposal step (Selective Search) is separate and not trainable as part of the network. This makes the entire process less cohesive.\n",
    "- **Faster R-CNN**: The RPN and Fast R-CNN components are trained jointly in an end-to-end manner. This unified approach allows for more seamless and efficient training and inference.\n",
    "\n",
    "### 3. **Shared Convolutional Layers**:\n",
    "\n",
    "- **Fast R-CNN**: Convolutional layers are used to extract features from the entire image, but the region proposals are generated separately.\n",
    "- **Faster R-CNN**: Shares convolutional layers between the RPN and the Fast R-CNN detection network. This sharing reduces computation redundancy and improves the efficiency of the network.\n",
    "\n",
    "### 4. **Anchor Boxes**:\n",
    "\n",
    "- **Fast R-CNN**: Relies on region proposals from methods like Selective Search, which may not be optimized for the specific features learned by the CNN.\n",
    "- **Faster R-CNN**: Introduces the concept of anchor boxes in RPN. Anchor boxes are predefined bounding boxes of different scales and aspect ratios that the RPN uses to generate proposals. This allows for more accurate and diverse region proposals.\n",
    "\n",
    "### 5. **Scalability and Flexibility**:\n",
    "\n",
    "- **Fast R-CNN**: The use of external region proposal methods makes it less flexible and slower to adapt to different datasets or tasks.\n",
    "- **Faster R-CNN**: The integrated RPN allows for easier adaptation and scalability to various datasets and tasks since the region proposal generation is directly influenced by the data and learning process.\n",
    "\n",
    "### 6. **Performance**:\n",
    "\n",
    "- **Fast R-CNN**: While it is faster than the original R-CNN, it is still limited by the speed and quality of the external region proposal methods.\n",
    "- **Faster R-CNN**: Achieves significant speed improvements over Fast R-CNN by integrating the region proposal step into the network. This integration results in faster and more accurate object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Explain the concept of anchor box\n",
    "\n",
    "The concept of anchor boxes (or anchor points) is integral to modern object detection frameworks like Faster R-CNN, SSD, and YOLO. Anchor boxes provide a mechanism for the network to predict bounding boxes of different sizes and aspect ratios, enabling more accurate localization of objects in images. Here’s a detailed explanation:\n",
    "\n",
    "### What are Anchor Boxes?\n",
    "\n",
    "Anchor boxes are predefined, fixed-sized bounding boxes that serve as reference points for predicting object locations and sizes. They are placed uniformly across the feature map, and the network learns to adjust these anchors to better fit the objects in the image.\n",
    "\n",
    "### How Anchor Boxes Work:\n",
    "\n",
    "1. **Predefined Sizes and Ratios**:\n",
    "   - Anchor boxes come in multiple sizes and aspect ratios. For example, you might define anchor boxes with three different scales (e.g., small, medium, large) and three different aspect ratios (e.g., 1:1, 2:1, 1:2). This gives you nine anchor boxes at each spatial location on the feature map.\n",
    "\n",
    "2. **Placement on Feature Map**:\n",
    "   - Anchor boxes are placed uniformly at each position on the feature map. If the feature map is of size \\( H \\times W \\), and you have \\( k \\) anchor boxes, you’ll have \\( H \\times W \\times k \\) anchor boxes in total.\n",
    "\n",
    "3. **Bounding Box Regression**:\n",
    "   - The network predicts adjustments to the anchor boxes to better fit the ground-truth bounding boxes of objects. These adjustments are in the form of offsets for the center coordinates, width, and height of the anchor boxes.\n",
    "\n",
    "4. **Objectness Score**:\n",
    "   - Alongside the adjustments, the network also predicts an objectness score for each anchor box, which indicates the likelihood that an object is present in the box. This score helps in filtering out anchor boxes that do not contain objects.\n",
    "\n",
    "### Mathematical Formulation:\n",
    "\n",
    "Assume an anchor box at position \\( (x_a, y_a) \\) with width \\( w_a \\) and height \\( h_a \\). The network predicts four values:\n",
    "- \\( t_x \\): Shift in the x-coordinate.\n",
    "- \\( t_y \\): Shift in the y-coordinate.\n",
    "- \\( t_w \\): Scale adjustment for the width.\n",
    "- \\( t_h \\): Scale adjustment for the height.\n",
    "\n",
    "The predicted bounding box coordinates \\( (x, y, w, h) \\) are computed as:\n",
    "\\[\n",
    "x = x_a + t_x \\cdot w_a\n",
    "\\]\n",
    "\\[\n",
    "y = y_a + t_y \\cdot h_a\n",
    "\\]\n",
    "\\[\n",
    "w = w_a \\cdot e^{t_w}\n",
    "\\]\n",
    "\\[\n",
    "h = h_a \\cdot e^{t_h}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Implement faster R-CNN using 2017 coco dataset i.e. train dataset, val dataset and test dataset. You can use a pre-trained backbone network like ResNet or VGG for feature extraction. For reference implement the following steps:\n",
    "\n",
    "a. Dataset preparation:\n",
    "download and preprocess the coco dataset, including annotations and images. \n",
    "split dataset into training and validation sets\n",
    "\n",
    "b. Model architecture:\n",
    "build a faster R-CNN model architecture using pre-trained backbone(eg-ResNet-50) for feature extraction\n",
    "Customize the RPN (region proposal network) and RCNN(region-based convolutional neural network) heads as necessary\n",
    "\n",
    "c. Training\n",
    "train faster R-CNN model on the training dataset\n",
    "implement a loss function that combines classification and regression losses\n",
    "utilise data augmentation techniques such as random cropping, flipping and scaling to improve model robustness\n",
    "\n",
    "d. Validation\n",
    "evaluate the trained model on validation dataset\n",
    "calculate and report evaluation metrics such as mAP(mean average precision) for object detection.\n",
    "\n",
    "e. Inference\n",
    "implement an inference pipeline to perform object detection on new images\n",
    "visualize the detected objects and their bounding boxes on test images\n",
    "\n",
    "f. Optional enhancements \n",
    "implement techniques like non-maximum suppression(NMS) to filter duplicate detections\n",
    "fine-tune the model or experiment with different backbone networks to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Define transforms for image preprocessing\n",
    "transform = Compose([\n",
    "    Resize((800, 800)),  # Resize images to a uniform size\n",
    "    ToTensor(),          # Convert PIL Image to PyTorch Tensor\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images\n",
    "])\n",
    "\n",
    "# Load COCO dataset\n",
    "train_dataset = CocoDetection(root='data/coco/train2017/',\n",
    "                              annFile='data/coco/annotations/instances_train2017.json',\n",
    "                              transform=transform)\n",
    "\n",
    "val_dataset = CocoDetection(root='data/coco/val2017/',\n",
    "                            annFile='data/coco/annotations/instances_val2017.json',\n",
    "                            transform=transform)\n",
    "\n",
    "# Split dataset into train and val\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# Load pre-trained ResNet-50 as backbone\n",
    "backbone = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# Customize anchor generator for RPN\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# Define RPN and RCNN heads\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)\n",
    "rpn_head = torchvision.models.detection.rpn.RPNHead(backbone.out_channels, anchor_generator.num_anchors_per_location()[0])\n",
    "rcnn_head = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(backbone.out_channels * 7 * 7, num_classes=len(train_dataset))\n",
    "\n",
    "# Combine backbone, RPN, and RCNN into Faster R-CNN model\n",
    "model = FasterRCNN(backbone, num_classes=len(train_dataset),\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   rpn_head=rpn_head,\n",
    "                   roi_pooler=roi_pooler,\n",
    "                   box_predictor=rcnn_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Define optimizer, scheduler, and device\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Train loop\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(5):  # Adjust number of epochs as needed\n",
    "    for images, targets in train_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# Save trained model\n",
    "torch.save(model.state_dict(), 'faster_rcnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation loop\n",
    "model.eval()\n",
    "\n",
    "# Evaluation loop\n",
    "for images, targets in val_loader:\n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(images)\n",
    "    \n",
    "    # Calculate metrics (e.g., mAP)\n",
    "    # Evaluate detections and calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def plot_image_with_boxes(image, boxes):\n",
    "    # Plot image\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # Add bounding boxes\n",
    "    for box in boxes:\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Load test image\n",
    "test_image = Image.open('path_to_test_image.jpg')\n",
    "test_image_tensor = transform(test_image).unsqueeze(0).to(device)\n",
    "\n",
    "# Perform inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_image_tensor)\n",
    "\n",
    "# Visualize detections\n",
    "boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "plot_image_with_boxes(test_image, boxes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
