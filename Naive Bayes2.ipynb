{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we use Bayes' theorem. Let's denote the events as follows:\n",
    "\n",
    "- Event A: An employee uses the company's health insurance plan.\n",
    "- Event B: An employee is a smoker.\n",
    "\n",
    "We are asked to find \\( P(B|A) \\), the probability that an employee is a smoker given that he/she uses the health insurance plan.\n",
    "\n",
    "According to Bayes' theorem:\n",
    "\n",
    "\\[ P(B|A) = \\frac{P(A|B) \\times P(B)}{P(A)} \\]\n",
    "\n",
    "Given:\n",
    "- \\( P(A) \\) = Probability that an employee uses the health insurance plan = 70% = 0.70\n",
    "- \\( P(B|A) \\) = Probability that an employee is a smoker given that he/she uses the health insurance plan (what we want to find)\n",
    "- \\( P(B) \\) = Probability that an employee is a smoker = 40% = 0.40\n",
    "\n",
    "We can calculate \\( P(A|B) \\), the probability that an employee uses the health insurance plan given that he/she is a smoker, using the provided information:\n",
    "- \\( P(A|B) \\) = Probability that an employee uses the health insurance plan given that he/she is a smoker = 40% = 0.40\n",
    "\n",
    "Now, let's calculate \\( P(B|A) \\) using Bayes' theorem:\n",
    "\n",
    "\\[ P(B|A) = \\frac{P(A|B) \\times P(B)}{P(A)} \\]\n",
    "\\[ P(B|A) = \\frac{0.40 \\times 0.40}{0.70} \\]\n",
    "\\[ P(B|A) = \\frac{0.16}{0.70} \\]\n",
    "\\[ P(B|A) â‰ˆ 0.2286 \\]\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.2286 or 22.86%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of features they are designed to handle and the underlying probability distributions they assume for those features:\n",
    "\n",
    "1. **Bernoulli Naive Bayes**:\n",
    "   - **Features**: Bernoulli Naive Bayes is typically used for binary feature data, where each feature can take on one of two possible values (e.g., presence or absence of a term in a document).\n",
    "   - **Probability Distribution**: It assumes that each feature follows a Bernoulli distribution, which is a discrete probability distribution for a binary random variable (0 or 1).\n",
    "   - **Example**: Bernoulli Naive Bayes is commonly used in text classification tasks, such as sentiment analysis or spam detection, where the presence or absence of certain words or features is used to classify documents.\n",
    "\n",
    "2. **Multinomial Naive Bayes**:\n",
    "   - **Features**: Multinomial Naive Bayes is suitable for categorical feature data, where each feature represents the count or frequency of a term occurring in a document or sample (e.g., word counts in a document).\n",
    "   - **Probability Distribution**: It assumes that each feature follows a multinomial distribution, which is a generalization of the binomial distribution to more than two possible outcomes.\n",
    "   - **Example**: Multinomial Naive Bayes is commonly used in text classification tasks, such as document categorization or topic classification, where the frequency of words or features in documents is used for classification.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes is used for binary feature data with Bernoulli-distributed features, while Multinomial Naive Bayes is used for categorical feature data with multinomial-distributed features. The choice between the two depends on the nature of the features and the assumptions that best match the data distribution in a given classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Bernoulli Naive Bayes handles missing values by simply ignoring them during the classification process. Since Bernoulli Naive Bayes assumes that each feature follows a Bernoulli distribution, which is a binary distribution representing the presence or absence of a feature, missing values are treated as absent features.\n",
    "\n",
    "During training, the presence or absence of each feature is determined based on whether it is present or absent in the training data. If a feature is missing for a particular sample in the training data, it is considered as absent, and its absence is accounted for in the probability calculations.\n",
    "\n",
    "Similarly, during classification, if a feature is missing for a new instance, it is also treated as absent. The classification algorithm calculates the probability of the instance belonging to each class based on the presence or absence of features and their associated probabilities, as learned from the training data.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes does not require imputation or special handling for missing values, as it naturally handles them by considering them as absent features. However, it's essential to ensure that missing values are properly encoded as such in the dataset before training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes that continuous features follow a Gaussian (normal) distribution. It is commonly used when the features are continuous and normally distributed.\n",
    "\n",
    "In multi-class classification, there are more than two classes to predict. Gaussian Naive Bayes can be adapted to handle multi-class classification by extending the underlying probability model to accommodate multiple classes. The algorithm calculates the likelihood of each class for a given instance based on the probability density function (PDF) of the Gaussian distribution for each feature in each class.\n",
    "\n",
    "During training, Gaussian Naive Bayes estimates the mean and variance of each feature for each class from the training data. Then, during classification, it computes the probability of each class given the observed feature values using Bayes' theorem and the Gaussian probability density function.\n",
    "\n",
    "In summary, Gaussian Naive Bayes can be used for both binary and multi-class classification tasks, making it a versatile algorithm for a wide range of machine learning problems. However, it assumes that the continuous features in the dataset are normally distributed, so it may not perform well if this assumption is violated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
      "0            0.00               0.64           0.64           0.0   \n",
      "1            0.21               0.28           0.50           0.0   \n",
      "2            0.06               0.00           0.71           0.0   \n",
      "3            0.00               0.00           0.00           0.0   \n",
      "4            0.00               0.00           0.00           0.0   \n",
      "\n",
      "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
      "0           0.32            0.00              0.00                0.00   \n",
      "1           0.14            0.28              0.21                0.07   \n",
      "2           1.23            0.19              0.19                0.12   \n",
      "3           0.63            0.00              0.31                0.63   \n",
      "4           0.63            0.00              0.31                0.63   \n",
      "\n",
      "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
      "0             0.00            0.00  ...         0.00        0.000   \n",
      "1             0.00            0.94  ...         0.00        0.132   \n",
      "2             0.64            0.25  ...         0.01        0.143   \n",
      "3             0.31            0.63  ...         0.00        0.137   \n",
      "4             0.31            0.63  ...         0.00        0.135   \n",
      "\n",
      "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
      "0          0.0        0.778        0.000        0.000   \n",
      "1          0.0        0.372        0.180        0.048   \n",
      "2          0.0        0.276        0.184        0.010   \n",
      "3          0.0        0.137        0.000        0.000   \n",
      "4          0.0        0.135        0.000        0.000   \n",
      "\n",
      "   capital_run_length_average  capital_run_length_longest  \\\n",
      "0                       3.756                          61   \n",
      "1                       5.114                         101   \n",
      "2                       9.821                         485   \n",
      "3                       3.537                          40   \n",
      "4                       3.537                          40   \n",
      "\n",
      "   capital_run_length_total  is_spam  \n",
      "0                       278        1  \n",
      "1                      1028        1  \n",
      "2                      2259        1  \n",
      "3                       191        1  \n",
      "4                       191        1  \n",
      "\n",
      "[5 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# URL of the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "\n",
    "# Column names for the dataset\n",
    "column_names = [\n",
    "    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n",
    "    \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n",
    "    \"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n",
    "    \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\n",
    "    \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\",\n",
    "    \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\",\n",
    "    \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\",\n",
    "    \"word_freq_pm\", \"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n",
    "    \"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\",\n",
    "    \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \"capital_run_length_average\", \"capital_run_length_longest\",\n",
    "    \"capital_run_length_total\", \"is_spam\"\n",
    "]\n",
    "\n",
    "# Read the dataset into a DataFrame\n",
    "df = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy (Bernoulli Naive Bayes): 0.8839380364047911\n",
      "Mean Accuracy (Multinomial Naive Bayes): 0.7863496180326323\n",
      "Mean Accuracy (Gaussian Naive Bayes): 0.8217730830896915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "# Split features and target variable\n",
    "X = df.drop(columns=[\"is_spam\"])\n",
    "y = df[\"is_spam\"]\n",
    "\n",
    "# Instantiate the Naive Bayes classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and calculate mean accuracy\n",
    "scores_bernoulli = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='accuracy')\n",
    "scores_multinomial = cross_val_score(multinomial_nb, X, y, cv=10, scoring='accuracy')\n",
    "scores_gaussian = cross_val_score(gaussian_nb, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "# Print mean accuracy for each classifier\n",
    "print(\"Mean Accuracy (Bernoulli Naive Bayes):\", scores_bernoulli.mean())\n",
    "print(\"Mean Accuracy (Multinomial Naive Bayes):\", scores_multinomial.mean())\n",
    "print(\"Mean Accuracy (Gaussian Naive Bayes):\", scores_gaussian.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metrics for Bernoulli Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91      2788\n",
      "           1       0.89      0.82      0.85      1813\n",
      "\n",
      "    accuracy                           0.89      4601\n",
      "   macro avg       0.89      0.87      0.88      4601\n",
      "weighted avg       0.89      0.89      0.88      4601\n",
      "\n",
      "Performance metrics for Multinomial Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83      2788\n",
      "           1       0.74      0.72      0.73      1813\n",
      "\n",
      "    accuracy                           0.79      4601\n",
      "   macro avg       0.78      0.78      0.78      4601\n",
      "weighted avg       0.79      0.79      0.79      4601\n",
      "\n",
      "Performance metrics for Gaussian Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.73      0.83      2788\n",
      "           1       0.70      0.96      0.81      1813\n",
      "\n",
      "    accuracy                           0.82      4601\n",
      "   macro avg       0.83      0.85      0.82      4601\n",
      "weighted avg       0.86      0.82      0.82      4601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define a function to calculate and print performance metrics\n",
    "def print_metrics(y_true, y_pred, classifier_name):\n",
    "    print(\"Performance metrics for\", classifier_name)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Perform predictions using each classifier and print performance metrics\n",
    "print_metrics(y, bernoulli_nb.fit(X, y).predict(X), \"Bernoulli Naive Bayes\")\n",
    "print_metrics(y, multinomial_nb.fit(X, y).predict(X), \"Multinomial Naive Bayes\")\n",
    "print_metrics(y, gaussian_nb.fit(X, y).predict(X), \"Gaussian Naive Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "Based on the results obtained from the performance metrics, we can discuss the performance of each variant of Naive Bayes:\n",
    "\n",
    "1. **Bernoulli Naive Bayes**:\n",
    "   - **Performance**: The accuracy, precision, recall, and F1 score of Bernoulli Naive Bayes are determined to evaluate its performance.\n",
    "   - **Observation**: Bernoulli Naive Bayes may perform well when dealing with binary features, such as presence or absence of certain words in text data.\n",
    "   - **Limitations**: Bernoulli Naive Bayes assumes that features are binary and independent, which may not always hold true in real-world datasets. Additionally, it may struggle with continuous or multinomial features.\n",
    "\n",
    "2. **Multinomial Naive Bayes**:\n",
    "   - **Performance**: The accuracy, precision, recall, and F1 score of Multinomial Naive Bayes are determined to evaluate its performance.\n",
    "   - **Observation**: Multinomial Naive Bayes may perform well when dealing with features representing counts or frequencies, such as word counts in text data.\n",
    "   - **Limitations**: Multinomial Naive Bayes assumes that features are multinomially distributed and independent, which may not always hold true in real-world datasets. It may also struggle with continuous or binary features.\n",
    "\n",
    "3. **Gaussian Naive Bayes**:\n",
    "   - **Performance**: The accuracy, precision, recall, and F1 score of Gaussian Naive Bayes are determined to evaluate its performance.\n",
    "   - **Observation**: Gaussian Naive Bayes may perform well when dealing with continuous features that follow a Gaussian distribution.\n",
    "   - **Limitations**: Gaussian Naive Bayes assumes that features are continuous and follow a Gaussian distribution, which may not hold true for all datasets. It may also struggle with categorical or binary features.\n",
    "\n",
    "Overall, the choice of which variant of Naive Bayes performs the best depends on the nature of the dataset and the characteristics of the features. In some cases, Bernoulli Naive Bayes may perform better if the features are binary, while in others, Multinomial or Gaussian Naive Bayes may be more appropriate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "In conclusion, we explored three variants of Naive Bayes classifiers (Bernoulli, Multinomial, and Gaussian) and evaluated their performance on the \"Spambase Data Set\" using 10-fold cross-validation. Here are the key findings:\n",
    "\n",
    "1. **Performance Comparison**: Each variant of Naive Bayes achieved different levels of performance based on accuracy, precision, recall, and F1 score metrics.\n",
    "2. **Best Performing Variant**: The best performing variant of Naive Bayes depended on the nature of the dataset and the characteristics of the features. Bernoulli Naive Bayes may perform well with binary features, Multinomial Naive Bayes with features representing counts or frequencies, and Gaussian Naive Bayes with continuous features following a Gaussian distribution.\n",
    "3. **Limitations**: Naive Bayes classifiers have certain limitations, such as the assumption of feature independence and the requirement for features to follow specific distributions, which may not always hold true in real-world datasets.\n",
    "4. **Future Work Suggestions**:\n",
    "   - Investigate feature engineering techniques to enhance the performance of Naive Bayes classifiers by transforming features or creating new ones that better capture the underlying patterns in the data.\n",
    "   - Explore ensemble methods that combine multiple Naive Bayes classifiers or integrate them with other machine learning algorithms to improve overall predictive performance.\n",
    "   - Evaluate Naive Bayes classifiers on diverse datasets with varying characteristics to gain insights into their strengths and weaknesses across different domains.\n",
    "   - Investigate techniques for handling imbalanced datasets, as Naive Bayes classifiers may struggle with class imbalances and biased predictions.\n",
    "\n",
    "Overall, Naive Bayes classifiers offer simplicity, efficiency, and interpretability, making them suitable for a wide range of classification tasks. However, understanding their limitations and exploring strategies to mitigate them can lead to more effective and robust machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
