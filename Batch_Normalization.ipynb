{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THEORY & CONCEPTS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the concept of batch normalization in the context of Artificial Neural Networks\n",
    "\n",
    "Batch normalization is a technique used in artificial neural networks to improve training speed, stability, and performance. It works by normalizing the inputs of each layer within a mini-batch, rather than normalizing the entire dataset.\n",
    "\n",
    "### Concept\n",
    "- **Normalization:** Batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation. This process ensures that the inputs to each layer have a mean of zero and a standard deviation of one.\n",
    "- **Scaling and Shifting:** After normalization, batch normalization applies a scaling (gamma) and shifting (beta) transformation. These parameters are learned during the training process, allowing the network to maintain the ability to represent the data flexibly.\n",
    "\n",
    "### Why Use Batch Normalization?\n",
    "1. **Improved Gradient Flow:** By maintaining a stable distribution of activations, batch normalization helps in preventing issues related to vanishing or exploding gradients, leading to better gradient flow through the network.\n",
    "2. **Higher Learning Rates:** It allows for the use of higher learning rates, which can speed up the convergence during training.\n",
    "3. **Regularization Effect:** It has a slight regularization effect, reducing the need for dropout or other regularization techniques.\n",
    "4. **Reduces Dependency on Initialization:** Batch normalization reduces the sensitivity of the network to the initial weights, making the network more robust to different initializations.\n",
    "\n",
    "### Implementation in Neural Networks\n",
    "Batch normalization can be applied to any layer within a neural network, typically after the activation function and before the next layer's input. In practice, it is often integrated into the layer definition in frameworks like TensorFlow or PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Describe the benefits of using batch normalization during training\n",
    "\n",
    "Batch normalization offers several benefits during the training of artificial neural networks, contributing to faster, more stable, and more efficient training processes. Here are the key benefits:\n",
    "\n",
    "### 1. **Improved Gradient Flow**\n",
    "- **Stabilizes Activations:** By normalizing the activations, batch normalization helps maintain a stable distribution of inputs to each layer, which ensures that the gradients during backpropagation remain well-behaved.\n",
    "- **Prevents Vanishing/Exploding Gradients:** This stability reduces the risk of gradients either vanishing or exploding, which is particularly beneficial in deep networks.\n",
    "\n",
    "### 2. **Enables Higher Learning Rates**\n",
    "- **Faster Convergence:** Normalized activations allow for the use of higher learning rates without the risk of the training process becoming unstable. Higher learning rates can lead to faster convergence, reducing the overall training time.\n",
    "\n",
    "### 3. **Reduces Internal Covariate Shift**\n",
    "- **Consistent Distribution:** Batch normalization mitigates the problem of internal covariate shift, where the distribution of inputs to each layer changes during training. By normalizing the inputs, the network adapts more quickly to new data distributions.\n",
    "\n",
    "### 4. **Acts as a Regularizer**\n",
    "- **Reduces Overfitting:** The mini-batch nature of normalization introduces a slight noise in the form of the batch statistics, which can have a regularizing effect, reducing the need for other regularization techniques like dropout.\n",
    "- **Improves Generalization:** This regularization can lead to better generalization on unseen data, improving the model’s performance on test sets.\n",
    "\n",
    "### 5. **Less Sensitive to Initialization**\n",
    "- **Robustness:** Batch normalization reduces the dependency on careful weight initialization, making the network more robust to different initial weight configurations. This robustness can simplify the model development process.\n",
    "\n",
    "### 6. **Reduces the Need for Other Forms of Data Normalization**\n",
    "- **Simplifies Data Preprocessing:** While data normalization is still important, batch normalization within the network layers can handle variations in the data distribution, potentially reducing the need for extensive data preprocessing.\n",
    "\n",
    "### 7. **Improves Training of Deeper Networks**\n",
    "- **Enables Deeper Architectures:** By addressing the issues of vanishing and exploding gradients and stabilizing learning, batch normalization makes it feasible to train much deeper networks effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters.\n",
    "\n",
    "### Working Principle\n",
    "\n",
    "#### Step 1: Compute the Mean and Variance\n",
    "For a given mini-batch, batch normalization first calculates the mean and variance of the inputs.\n",
    "\n",
    "- **Mini-batch Mean (\\(\\mu_B\\))**: For each feature dimension, compute the mean of the inputs in the mini-batch.\n",
    "  \\[\n",
    "  \\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\n",
    "  \\]\n",
    "  where \\(m\\) is the number of examples in the mini-batch.\n",
    "\n",
    "- **Mini-batch Variance (\\(\\sigma_B^2\\))**: For each feature dimension, compute the variance of the inputs in the mini-batch.\n",
    "  \\[\n",
    "  \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\n",
    "  \\]\n",
    "\n",
    "#### Step 2: Normalize the Batch\n",
    "Using the computed mean and variance, normalize the inputs of the mini-batch.\n",
    "\n",
    "- **Normalization**: Subtract the mean and divide by the square root of the variance (plus a small constant \\(\\epsilon\\) for numerical stability).\n",
    "  \\[\n",
    "  \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "  \\]\n",
    "\n",
    "#### Step 3: Scale and Shift\n",
    "After normalization, batch normalization applies a linear transformation to maintain the representational power of the network. This transformation includes two learnable parameters: gamma (\\(\\gamma\\)) and beta (\\(\\beta\\)).\n",
    "\n",
    "- **Scaling (\\(\\gamma\\))**: Multiply the normalized value by a learnable scale parameter.\n",
    "- **Shifting (\\(\\beta\\))**: Add a learnable shift parameter.\n",
    "\n",
    "  \\[\n",
    "  y_i = \\gamma \\hat{x}_i + \\beta\n",
    "  \\]\n",
    "\n",
    "Here, \\(\\gamma\\) and \\(\\beta\\) are learned during the training process. They allow the network to undo the normalization if it is beneficial for learning.\n",
    "\n",
    "### Summary of Steps\n",
    "1. **Compute mean (\\(\\mu_B\\)) and variance (\\(\\sigma_B^2\\)) for the mini-batch.**\n",
    "2. **Normalize the inputs (\\(\\hat{x}_i\\)) using the computed mean and variance.**\n",
    "3. **Apply learnable scale (\\(\\gamma\\)) and shift (\\(\\beta\\)) parameters to the normalized inputs.**\n",
    "\n",
    "### Learnable Parameters\n",
    "- **Gamma (\\(\\gamma\\))**: Controls the scaling of the normalized inputs.\n",
    "- **Beta (\\(\\beta\\))**: Controls the shifting of the normalized inputs.\n",
    "\n",
    "These parameters are learned during the training process through backpropagation, just like other parameters (weights and biases) in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENTATION:\n",
    "\n",
    "1. Choose a dataset of your choice (e.g., MNIST, CIAR-0) and preprocess itr\n",
    "2. Implement a simple feedforward neural network using any deep learning framework/library (e.g.,Tensorlow, xyTorch)r\n",
    "3. Train the neural network on the chosen dataset without using batch normalization\n",
    "4. Implement batch normalization layers in the neural network and train the model again\n",
    "5. Compare the training and validation performance (e.g., accuracy, loss) between the models with and without batch normalization\n",
    "6. Discuss the impact of batch normalization on the training process and the performance of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:02<00:00, 3867215.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 141092.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:02<00:00, 732417.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Training without batch normalization\n",
      "Epoch 1, Loss: 1.0209853329193364\n",
      "Epoch 2, Loss: 0.38007229391827\n",
      "Epoch 3, Loss: 0.32163135676400495\n",
      "Epoch 4, Loss: 0.29001505731710236\n",
      "Epoch 5, Loss: 0.2651524154235051\n",
      "Accuracy: 92.91\n"
     ]
    }
   ],
   "source": [
    "#preprocess MNIST dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the transformations for the dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training the neural network\n",
    "def train(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluating the neural network\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {100 * correct / total}\")\n",
    "\n",
    "# Train and evaluate the model without batch normalization\n",
    "print(\"Training without batch normalization\")\n",
    "train(model, train_loader, criterion, optimizer)\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch normalization\n",
      "Epoch 1, Loss: 0.47388223089230086\n",
      "Epoch 2, Loss: 0.18839831663363144\n",
      "Epoch 3, Loss: 0.1338330885943478\n",
      "Epoch 4, Loss: 0.10305348453300595\n",
      "Epoch 5, Loss: 0.08224234271691298\n",
      "Accuracy: 97.41\n"
     ]
    }
   ],
   "source": [
    "class SimpleNNWithBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNWithBN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer for the network with batch normalization\n",
    "model_bn = SimpleNNWithBN()\n",
    "optimizer_bn = optim.SGD(model_bn.parameters(), lr=0.01)\n",
    "\n",
    "# Train and evaluate the model with batch normalization\n",
    "print(\"Training with batch normalization\")\n",
    "train(model_bn, train_loader, criterion, optimizer_bn)\n",
    "evaluate(model_bn, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the training and validation performance between models with and without batch normalization, several theoretical differences can be observed:\n",
    "\n",
    "### Training Performance\n",
    "\n",
    "#### Without Batch Normalization:\n",
    "- **Training Loss:** The training loss may fluctuate more and decrease at a slower rate due to the internal covariate shift, where the distribution of each layer's inputs changes during training.\n",
    "- **Training Accuracy:** The training accuracy might increase slowly and could plateau earlier, indicating slower learning.\n",
    "\n",
    "#### With Batch Normalization:\n",
    "- **Training Loss:** The training loss tends to decrease more rapidly and steadily because batch normalization stabilizes the input distributions, allowing for higher learning rates and more efficient training.\n",
    "- **Training Accuracy:** The training accuracy typically improves faster and continues to increase for more epochs, indicating more effective learning.\n",
    "\n",
    "### Validation Performance\n",
    "\n",
    "#### Without Batch Normalization:\n",
    "- **Validation Loss:** The validation loss might be higher and more variable, reflecting poorer generalization due to the model's overfitting to the training data or instability in training.\n",
    "- **Validation Accuracy:** The validation accuracy could be lower and less consistent, indicating that the model does not generalize well to unseen data.\n",
    "\n",
    "#### With Batch Normalization:\n",
    "- **Validation Loss:** The validation loss is generally lower and more stable, suggesting better generalization. Batch normalization acts as a regularizer, reducing the risk of overfitting.\n",
    "- **Validation Accuracy:** The validation accuracy tends to be higher and more consistent, reflecting improved generalization and robustness to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPERIMENTATION & ANALYSIS\n",
    "\n",
    "1. Experiment with different batch sizes and observe the effect on the training dynamics and model performance\n",
    "2. Discuss the advantages and potential limitations of batch normalization in improving the training of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to run experiments with different batch sizes\n",
    "def run_experiment(batch_size, use_batch_norm=False):\n",
    "    # Load the dataset with the specified batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "    # Choose the model architecture based on the use_batch_norm flag\n",
    "    if use_batch_norm:\n",
    "        model = SimpleNNWithBN()\n",
    "    else:\n",
    "        model = SimpleNN()\n",
    "\n",
    "    # Initialize the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Train the model\n",
    "    print(f\"Training with batch size {batch_size} {'with batch normalization' if use_batch_norm else 'without batch normalization'}\")\n",
    "    train(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = evaluate(model, test_loader)\n",
    "    return accuracy\n",
    "\n",
    "# Run experiments with different batch sizes without batch normalization\n",
    "for batch_size in [32, 64, 128]:\n",
    "    accuracy = run_experiment(batch_size, use_batch_norm=False)\n",
    "    print(f\"Final Test Accuracy without Batch Normalization for batch size {batch_size}: {accuracy}%\")\n",
    "\n",
    "# Run experiments with different batch sizes with batch normalization\n",
    "for batch_size in [32, 64, 128]:\n",
    "    accuracy = run_experiment(batch_size, use_batch_norm=True)\n",
    "    print(f\"Final Test Accuracy with Batch Normalization for batch size {batch_size}: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of Batch Normalization:\n",
    "\n",
    "1. **Accelerates Training:** By normalizing inputs, batch normalization allows the use of higher learning rates, which speeds up convergence.\n",
    "2. **Stabilizes Training:** It mitigates the issues of vanishing/exploding gradients by ensuring that inputs to each layer are normalized.\n",
    "3. **Regularization Effect:** The mini-batch statistics introduce noise, which helps in regularizing the model and reducing overfitting.\n",
    "4. **Improves Generalization:** Better stability and regularization typically result in improved performance on unseen data.\n",
    "5. **Less Sensitivity to Initialization:** The model becomes less sensitive to weight initialization, simplifying the training process.\n",
    "\n",
    "#### Potential Limitations:\n",
    "\n",
    "1. **Dependency on Batch Size:** The effectiveness of batch normalization can be sensitive to the batch size. Too small batch sizes can lead to inaccurate statistics, while too large batch sizes may reduce the regularization effect.\n",
    "2. **Additional Computation:** Batch normalization introduces additional computations during training, which can increase the overall training time, although this is often offset by faster convergence.\n",
    "3. **Complexity in Recurrent Neural Networks:** Applying batch normalization in recurrent neural networks (RNNs) can be more complex and might not always provide the same benefits as in feedforward networks.\n",
    "4. **Inference Time:** During inference, the normalization statistics need to be fixed, which requires careful handling of the moving averages of mean and variance computed during training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
